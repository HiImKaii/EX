import numpy as np
import pandas as pd
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

class SmartDataBalancer:
    def __init__(self):
        # ==== B·ªò THAM S·ªê ƒêI·ªÄU CH·ªàNH ====
        self.params = {
            # Ph√°t hi·ªán outlier (nh∆∞ c≈©)
            'zscore_threshold': 3.5,
            'iqr_multiplier': 2.0,
            'isolation_contamination': 0.05,
            
            # X√°c th·ª±c outlier (nh∆∞ c≈©)
            'max_noise_rate': 0.12,
            'min_improvement': 0.005,
            'min_samples': 30,
            
            # Normality test (nh∆∞ c≈©)
            'normality_alpha': 0.01,
            'max_skew': 2.5,
            'max_kurt': 8.0,
            
            # Consensus (nh∆∞ c≈©)
            'min_votes': 2,
            'max_iterations': 2,
            
            # ==== THAM S·ªê M·ªöI CHO BALANCING ====
            'target_ratio': 3.0,           # T·ª∑ l·ªá m·ª•c ti√™u non-flood:flood (3:1)
            'clustering_ratio': 0.3,       # 30% d√πng clustering
            'boundary_ratio': 0.3,         # 30% d√πng boundary analysis
            'random_ratio': 0.4,           # 40% random (sau khi l·ªçc noise)
            'min_cluster_size': 5,         # K√≠ch th∆∞·ªõc cluster t·ªëi thi·ªÉu
            'boundary_neighbors': 10,      # S·ªë l√°ng gi·ªÅng cho boundary analysis
            'preserve_outliers': True      # Gi·ªØ l·∫°i outliers c√≥ th·ªÉ quan tr·ªçng
        }
    
    def is_normal_distributed(self, data):
        """Ki·ªÉm tra ph√¢n ph·ªëi chu·∫©n v·ªõi multiple tests"""
        if len(data) < 8:
            return False, 0
        
        try:
            _, jb_p = stats.jarque_bera(data)
            _, sw_p = stats.shapiro(data) if len(data) < 5000 else (0, 0.5)
            _, da_p = stats.normaltest(data)
            
            p_values = [p for p in [jb_p, sw_p, da_p] if p > 0]
            avg_p = np.mean(p_values) if p_values else 0
            
            skew = abs(stats.skew(data))
            kurt = abs(stats.kurtosis(data))
            
            is_normal = (avg_p > self.params['normality_alpha'] and 
                        skew < self.params['max_skew'] and 
                        kurt < self.params['max_kurt'])
            
            return is_normal, avg_p
        except:
            return False, 0
    
    def detect_outliers_consensus(self, data):
        """Ph√°t hi·ªán outliers b·∫±ng consensus voting"""
        if len(data) < self.params['min_samples']:
            return pd.Series([False] * len(data), index=data.index)
        
        outlier_votes = []
        
        # Method 1: Modified Z-score
        try:
            median_val = data.median()
            mad = np.median(np.abs(data - median_val))
            if mad > 0:
                mod_z = 0.6745 * (data - median_val) / mad
                outlier_votes.append(np.abs(mod_z) > self.params['zscore_threshold'])
        except:
            pass
        
        # Method 2: IQR method
        try:
            Q1, Q3 = data.quantile([0.25, 0.75])
            IQR = Q3 - Q1
            if IQR > 0:
                lower = Q1 - self.params['iqr_multiplier'] * IQR
                upper = Q3 + self.params['iqr_multiplier'] * IQR
                outlier_votes.append((data < lower) | (data > upper))
        except:
            pass
        
        # Method 3: Isolation Forest
        if len(data.dropna()) >= 50:
            try:
                iso_forest = IsolationForest(
                    contamination=self.params['isolation_contamination'],
                    random_state=42,
                    n_estimators=50
                )
                clean_data = data.dropna()
                predictions = iso_forest.fit_predict(clean_data.values.reshape(-1, 1))
                iso_outliers = pd.Series([False] * len(data), index=data.index)
                iso_outliers.loc[clean_data.index] = (predictions == -1)
                outlier_votes.append(iso_outliers)
            except:
                pass
        
        if len(outlier_votes) == 0:
            return pd.Series([False] * len(data), index=data.index)
        
        vote_matrix = pd.DataFrame(outlier_votes).T.fillna(False)
        consensus = vote_matrix.sum(axis=1) >= self.params['min_votes']
        
        return pd.Series(consensus, index=data.index)
    
    def validate_outliers(self, data, outliers):
        """X√°c th·ª±c outliers ƒë·ªÉ tr√°nh over-filtering"""
        if outliers.sum() == 0:
            return outliers
        
        noise_rate = outliers.sum() / len(data)
        
        if noise_rate > self.params['max_noise_rate']:
            median_val = data.median()
            mad = np.median(np.abs(data - median_val))
            if mad > 0:
                extreme_z = np.abs(0.6745 * (data - median_val) / mad)
                threshold = np.percentile(extreme_z, 98)
                return extreme_z > threshold
            return pd.Series([False] * len(data), index=data.index)
        
        # B·∫£o v·ªá consecutive patterns
        validated = outliers.copy()
        outlier_indices = data[outliers].index.tolist()
        
        if len(outlier_indices) > 3:
            for i, idx in enumerate(outlier_indices[:-1]):
                cluster_size = 1
                for j in range(i+1, len(outlier_indices)):
                    if outlier_indices[j] - outlier_indices[j-1] <= 2:
                        cluster_size += 1
                    else:
                        break
                
                if cluster_size > 4:
                    cluster_indices = outlier_indices[i:i+cluster_size]
                    for cluster_idx in cluster_indices:
                        validated.loc[cluster_idx] = False
        
        return validated
    
    def identify_cluster_representatives(self, df_non_flood, n_samples):
        """S·ª≠ d·ª•ng clustering ƒë·ªÉ t√¨m ƒëi·ªÉm ƒë·∫°i di·ªán"""
        print(f"  üîç Clustering analysis...")
        
        numeric_cols = df_non_flood.select_dtypes(include=[np.number]).columns.tolist()
        if not numeric_cols:
            return df_non_flood.sample(n_samples, random_state=42)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(df_non_flood[numeric_cols].fillna(0))
        
        # X√°c ƒë·ªãnh s·ªë cluster t·ªëi ∆∞u
        n_clusters = max(self.params['min_cluster_size'], 
                        min(n_samples // self.params['min_cluster_size'], len(df_non_flood) // 20))
        
        # Clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)
        
        # Ch·ªçn ƒëi·ªÉm ƒë·∫°i di·ªán t·ª´ m·ªói cluster
        representatives = []
        samples_per_cluster = n_samples // n_clusters
        extra_samples = n_samples % n_clusters
        
        for i in range(n_clusters):
            cluster_mask = (clusters == i)
            cluster_data = df_non_flood[cluster_mask]
            
            if len(cluster_data) == 0:
                continue
            
            # S·ªë m·∫´u cho cluster n√†y
            cluster_samples = samples_per_cluster + (1 if i < extra_samples else 0)
            cluster_samples = min(cluster_samples, len(cluster_data))
            
            if cluster_samples == 1:
                # Ch·ªçn ƒëi·ªÉm g·∫ßn centroid nh·∫•t
                cluster_center = kmeans.cluster_centers_[i]
                cluster_X = X_scaled[cluster_mask]
                distances = np.sum((cluster_X - cluster_center) ** 2, axis=1)
                closest_idx = cluster_data.iloc[np.argmin(distances)].name
                representatives.append(closest_idx)
            else:
                # Ch·ªçn nhi·ªÅu ƒëi·ªÉm ƒë·∫°i di·ªán
                selected = cluster_data.sample(cluster_samples, random_state=42)
                representatives.extend(selected.index.tolist())
        
        print(f"    ‚úÖ ƒê√£ ch·ªçn {len(representatives)} ƒëi·ªÉm t·ª´ {n_clusters} clusters")
        return df_non_flood.loc[representatives]
    
    def identify_boundary_samples(self, df_non_flood, df_flood, n_samples):
        """T√¨m c√°c m·∫´u non-flood g·∫ßn boundary v·ªõi flood"""
        print(f"  üéØ Boundary analysis...")
        
        numeric_cols = df_non_flood.select_dtypes(include=[np.number]).columns.tolist()
        if not numeric_cols or len(df_flood) == 0:
            return df_non_flood.sample(n_samples, random_state=42)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaler = StandardScaler()
        
        # Fit tr√™n to√†n b·ªô d·ªØ li·ªáu
        all_data = pd.concat([df_non_flood[numeric_cols], df_flood[numeric_cols]])
        scaler.fit(all_data.fillna(0))
        
        X_non_flood = scaler.transform(df_non_flood[numeric_cols].fillna(0))
        X_flood = scaler.transform(df_flood[numeric_cols].fillna(0))
        
        # T√¨m k nearest neighbors t·ª´ flood data
        k = min(self.params['boundary_neighbors'], len(df_flood))
        nn = NearestNeighbors(n_neighbors=k)
        nn.fit(X_flood)
        
        # T√≠nh kho·∫£ng c√°ch ƒë·∫øn flood samples
        distances, _ = nn.kneighbors(X_non_flood)
        avg_distances = np.mean(distances, axis=1)
        
        # Ch·ªçn nh·ªØng ƒëi·ªÉm g·∫ßn boundary nh·∫•t
        boundary_indices = np.argsort(avg_distances)[:n_samples]
        selected_samples = df_non_flood.iloc[boundary_indices]
        
        print(f"    ‚úÖ ƒê√£ ch·ªçn {len(selected_samples)} ƒëi·ªÉm g·∫ßn boundary")
        return selected_samples
    
    def smart_balance(self, df, flood_column='flood', target_total=2000):
        """C√¢n b·∫±ng d·ªØ li·ªáu th√¥ng minh"""
        print(f"üìä SMART DATA BALANCING")
        print(f"   Target: {target_total} total samples")
        
        # Ph√¢n t√°ch d·ªØ li·ªáu
        df_flood = df[df[flood_column] == 1].copy()
        df_non_flood = df[df[flood_column] == 0].copy()
        
        n_flood = len(df_flood)
        n_non_flood = len(df_non_flood)
        
        print(f"   Hi·ªán t·∫°i: {n_flood} flood, {n_non_flood} non-flood")
        
        # T√≠nh to√°n target
        target_flood = n_flood  # Gi·ªØ nguy√™n s·ªë l∆∞·ª£ng flood
        target_non_flood = target_total - target_flood
        
        if target_non_flood >= n_non_flood:
            print(f"   ‚ö†Ô∏è  Target non-flood ({target_non_flood}) >= hi·ªán t·∫°i ({n_non_flood})")
            print(f"   S·∫Ω gi·ªØ nguy√™n v√† ch·ªâ l·ªçc noise...")
            return self.filter_noise_only(df)
        
        print(f"   Target: {target_flood} flood, {target_non_flood} non-flood")
        print(f"   C·∫ßn gi·∫£m: {n_non_flood - target_non_flood} non-flood samples")
        
        # B∆Ø·ªöC 1: L·ªçc noise t·ª´ non-flood tr∆∞·ªõc
        print(f"\nüîß B∆Ø·ªöC 1: L·ªçc noise t·ª´ non-flood data...")
        df_non_flood_clean = self.filter_noise_targeted(df_non_flood)
        n_after_noise = len(df_non_flood_clean)
        print(f"   Sau l·ªçc noise: {n_after_noise} non-flood samples")
        
        if n_after_noise <= target_non_flood:
            print(f"   ‚úÖ ƒê√£ ƒë·∫°t target sau l·ªçc noise!")
            return pd.concat([df_flood, df_non_flood_clean]).reset_index(drop=True)
        
        # B∆Ø·ªöC 2: √Åp d·ª•ng chi·∫øn l∆∞·ª£c balancing
        print(f"\nüéØ B∆Ø·ªöC 2: Smart sampling...")
        remaining_to_remove = n_after_noise - target_non_flood
        
        # Ph√¢n b·ªï theo t·ª∑ l·ªá
        n_cluster = int(target_non_flood * self.params['clustering_ratio'])
        n_boundary = int(target_non_flood * self.params['boundary_ratio'])  
        n_random = target_non_flood - n_cluster - n_boundary
        
        print(f"   Ph√¢n b·ªï: {n_cluster} (cluster) + {n_boundary} (boundary) + {n_random} (random)")
        
        selected_samples = []
        
        # Clustering-based selection
        if n_cluster > 0:
            cluster_samples = self.identify_cluster_representatives(df_non_flood_clean, n_cluster)
            selected_samples.append(cluster_samples)
        
        # Boundary-based selection  
        if n_boundary > 0:
            boundary_samples = self.identify_boundary_samples(df_non_flood_clean, df_flood, n_boundary)
            selected_samples.append(boundary_samples)
        
        # Random selection t·ª´ ph·∫ßn c√≤n l·∫°i
        if n_random > 0:
            used_indices = set()
            for sample in selected_samples:
                used_indices.update(sample.index)
            
            remaining_data = df_non_flood_clean.drop(index=list(used_indices))
            if len(remaining_data) > 0:
                n_random_actual = min(n_random, len(remaining_data))
                random_samples = remaining_data.sample(n_random_actual, random_state=42)
                selected_samples.append(random_samples)
        
        # K·∫øt h·ª£p t·∫•t c·∫£
        final_non_flood = pd.concat(selected_samples).drop_duplicates()
        
        # ƒê·∫£m b·∫£o ƒë√∫ng s·ªë l∆∞·ª£ng
        if len(final_non_flood) > target_non_flood:
            final_non_flood = final_non_flood.sample(target_non_flood, random_state=42)
        
        # K·∫øt qu·∫£ cu·ªëi c√πng
        result = pd.concat([df_flood, final_non_flood]).reset_index(drop=True)
        
        print(f"\nüìà K·∫æT QU·∫¢ BALANCING:")
        print(f"   ‚Ä¢ Flood: {len(df_flood)} (gi·ªØ nguy√™n)")
        print(f"   ‚Ä¢ Non-flood: {n_non_flood} ‚Üí {len(final_non_flood)} (-{n_non_flood - len(final_non_flood)})")
        print(f"   ‚Ä¢ T·ªïng: {len(df)} ‚Üí {len(result)} ({len(result)/len(df):.1%})")
        print(f"   ‚Ä¢ T·ª∑ l·ªá: 1:{len(final_non_flood)/len(df_flood):.1f}")
        
        return result
    
    def filter_noise_targeted(self, df):
        """L·ªçc noise c√≥ m·ª•c ti√™u c·ª• th·ªÉ"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if not numeric_cols:
            return df
        
        current_df = df.copy()
        total_removed = 0
        
        for iteration in range(self.params['max_iterations']):
            iteration_outliers = pd.Series([False] * len(current_df), index=current_df.index)
            any_improvement = False
            
            for col in numeric_cols:
                col_data = current_df[col].dropna()
                if len(col_data) < self.params['min_samples']:
                    continue
                
                is_normal_before, p_before = self.is_normal_distributed(col_data)
                if is_normal_before:
                    continue
                
                outliers = self.detect_outliers_consensus(current_df[col])
                validated_outliers = self.validate_outliers(current_df[col], outliers)
                
                if validated_outliers.sum() == 0:
                    continue
                
                temp_data = current_df.loc[~validated_outliers, col].dropna()
                if len(temp_data) >= self.params['min_samples']:
                    is_normal_after, p_after = self.is_normal_distributed(temp_data)
                    improvement = p_after - p_before
                    
                    if improvement > self.params['min_improvement']:
                        iteration_outliers = iteration_outliers | validated_outliers
                        any_improvement = True
            
            if not any_improvement:
                break
            
            removed_this_iter = iteration_outliers.sum()
            current_df = current_df[~iteration_outliers].copy()
            total_removed += removed_this_iter
        
        print(f"    ƒê√£ l·ªçc {total_removed} noise samples")
        return current_df
    
    def filter_noise_only(self, df):
        """Ch·ªâ l·ªçc noise, kh√¥ng balance"""
        print(f"üìä CH·ªà L·ªåC NOISE - KH√îNG BALANCE")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if not numeric_cols:
            return df
        
        current_df = df.copy()
        total_removed = 0
        
        for iteration in range(self.params['max_iterations']):
            print(f"\nüîÑ L·∫ßn l·∫∑p {iteration + 1}:")
            
            iteration_outliers = pd.Series([False] * len(current_df), index=current_df.index)
            any_improvement = False
            
            for col in numeric_cols:
                col_data = current_df[col].dropna()
                if len(col_data) < self.params['min_samples']:
                    continue
                
                is_normal_before, p_before = self.is_normal_distributed(col_data)
                if is_normal_before:
                    print(f"  ‚úÖ {col}: ƒê√£ chu·∫©n (p={p_before:.3f})")
                    continue
                
                outliers = self.detect_outliers_consensus(current_df[col])
                validated_outliers = self.validate_outliers(current_df[col], outliers)
                
                if validated_outliers.sum() == 0:
                    continue
                
                temp_data = current_df.loc[~validated_outliers, col].dropna()
                if len(temp_data) >= self.params['min_samples']:
                    is_normal_after, p_after = self.is_normal_distributed(temp_data)
                    improvement = p_after - p_before
                    
                    if improvement > self.params['min_improvement']:
                        iteration_outliers = iteration_outliers | validated_outliers
                        any_improvement = True
                        noise_rate = validated_outliers.sum() / len(current_df)
                        print(f"  üéØ {col}: {validated_outliers.sum()} outliers ({noise_rate:.2%}) | "
                              f"p: {p_before:.3f} ‚Üí {p_after:.3f}")
            
            if not any_improvement:
                print(f"  üéâ H·ªôi t·ª•! Kh√¥ng c√≤n c·∫£i thi·ªán ƒë∆∞·ª£c n·ªØa.")
                break
            
            removed_this_iter = iteration_outliers.sum()
            current_df = current_df[~iteration_outliers].copy()
            total_removed += removed_this_iter
            print(f"  üìâ Lo·∫°i b·ªè: {removed_this_iter} h√†ng")
        
        preservation_rate = len(current_df) / len(df)
        print(f"\nüìà K·∫æT QU·∫¢ CU·ªêI C√ôNG:")
        print(f"   ‚Ä¢ Lo·∫°i b·ªè: {total_removed} h√†ng ({total_removed/len(df):.1%})")
        print(f"   ‚Ä¢ B·∫£o to√†n: {preservation_rate:.1%} d·ªØ li·ªáu")
        print(f"   ‚Ä¢ K√≠ch th∆∞·ªõc: {df.shape} ‚Üí {current_df.shape}")
        
        return current_df


def balance_flood_data(file_path, flood_column='flood', target_total=2000, custom_params=None):
    """
    H√†m ch√≠nh - C√¢n b·∫±ng d·ªØ li·ªáu l≈© th√¥ng minh
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n file CSV
        flood_column: T√™n c·ªôt ch·ª©a label l≈© (1=l≈©, 0=kh√¥ng l≈©)
        target_total: T·ªïng s·ªë m·∫´u m·ª•c ti√™u
        custom_params: Dict tham s·ªë t√πy ch·ªânh
    
    Returns:
        DataFrame ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng
    """
    print("üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
    df = pd.read_csv(file_path)
    
    # Ki·ªÉm tra c·ªôt flood
    if flood_column not in df.columns:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt '{flood_column}' trong d·ªØ li·ªáu!")
        print(f"Available columns: {list(df.columns)}")
        return df
    
    # Ki·ªÉm tra gi√° tr·ªã trong c·ªôt flood
    flood_values = df[flood_column].unique()
    print(f"üìä Gi√° tr·ªã trong c·ªôt '{flood_column}': {flood_values}")
    
    if not set(flood_values).issubset({0, 1}):
        print(f"‚ö†Ô∏è  C·ªôt '{flood_column}' ch·ª©a gi√° tr·ªã kh√¥ng ph·∫£i 0/1: {flood_values}")
        print("S·∫Ω chuy·ªÉn ƒë·ªïi: 0 = kh√¥ng l≈©, kh√°c 0 = l≈©")
        df[flood_column] = (df[flood_column] != 0).astype(int)
    
    # Kh·ªüi t·∫°o balancer
    balancer = SmartDataBalancer()
    
    # √Åp d·ª•ng tham s·ªë t√πy ch·ªânh
    if custom_params:
        balancer.params.update(custom_params)
        print(f"‚öôÔ∏è  ƒê√£ c·∫≠p nh·∫≠t tham s·ªë: {custom_params}")
    
    # Th·ª±c hi·ªán c√¢n b·∫±ng
    df_balanced = balancer.smart_balance(df, flood_column, target_total)
    
    # L∆∞u k·∫øt qu·∫£
    output_path = file_path.replace('.csv', '_balanced.csv')
    df_balanced.to_csv(output_path, index=False)
    print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£: {output_path}")
    
    return df_balanced


# C√ÅCH S·ª¨ D·ª§NG
if __name__ == "__main__":
    # S·ª≠ d·ª•ng c∆° b·∫£n - c√¢n b·∫±ng xu·ªëng 2000 m·∫´u
    file_path = r"D:\Vscode\flood_point_merge.csv"
    df_balanced = balance_flood_data(file_path, flood_column='flood', target_total=2000)
    
    # T√πy ch·ªânh c√°c tham s·ªë
    # custom_params = {
    #     'target_ratio': 2.5,           # T·ª∑ l·ªá non-flood:flood = 2.5:1
    #     'clustering_ratio': 0.4,       # 40% d√πng clustering
    #     'boundary_ratio': 0.4,         # 40% d√πng boundary analysis
    #     'random_ratio': 0.2,           # 20% random
    #     'zscore_threshold': 3.0,       # Nghi√™m ng·∫∑t h∆°n trong l·ªçc noise
    # }
    # df_balanced = balance_flood_data(file_path, 'flood', 2000, custom_params)