import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.ensemble import RandomForestRegressor
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import psutil
import time
import gc
import os
import warnings
warnings.filterwarnings('ignore')

def quick_impute_large_csv(input_file, output_file, chunk_size=1000000, n_workers=20):
    """
    N·ªôi suy nhanh cho file CSV l·ªõn v·ªõi multiprocessing - T·ªëi ∆∞u cao
    
    Args:
        input_file: ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu v√†o
        output_file: ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        chunk_size: K√≠ch th∆∞·ªõc chunk (1M cho m√°y m·∫°nh)
        n_workers: S·ªë workers (20 cho m√°y 24 cores)
    """
    print(f"üöÄ === N·ªòI SUY FILE L·ªöN V·ªöI MULTIPROCESSING ===")
    print(f"üíª C·∫•u h√¨nh: {n_workers} workers, chunk size {chunk_size:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    print(f"üìÅ File: {input_file}")
    
    # Ki·ªÉm tra file t·ªìn t·∫°i
    if not os.path.exists(input_file):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {input_file}")
        return False
    
    # ƒê·ªçc m·∫´u ƒë·ªÉ ki·ªÉm tra missing data
    print("üìä Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu...")
    sample = pd.read_csv(input_file, nrows=10000)
    missing_info = sample.isnull().sum()
    missing_cols = missing_info[missing_info > 0]
    
    if len(missing_cols) == 0:
        print("‚úÖ Kh√¥ng c√≥ d·ªØ li·ªáu thi·∫øu!")
        return True
    
    print(f"üîß T√¨m th·∫•y {len(missing_cols)} c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu:")
    for col, count in missing_cols.items():
        pct = (count / len(sample)) * 100
        print(f"   {col}: {pct:.1f}%")
    
    # X·ª≠ l√Ω t·ª´ng chunk v·ªõi multiprocessing
    print("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω song song...")
    start_time = time.time()
    first_chunk = True
    total_processed = 0
    
    # Batch processing
    batch_size = n_workers * 2
    chunk_batch = []
    
    try:
        chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, engine='c', buffer_lines=50000)
        
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            for i, chunk in enumerate(chunk_reader):
                chunk_batch.append(chunk)
                
                # X·ª≠ l√Ω batch khi ƒë·∫ßy
                if len(chunk_batch) >= batch_size:
                    # Submit batch ƒë·ªÉ x·ª≠ l√Ω song song
                    futures = [executor.submit(fast_impute_chunk, chunk_data) for chunk_data in chunk_batch]
                    
                    # Thu th·∫≠p k·∫øt qu·∫£ v√† ghi file
                    batch_results = []
                    for future in as_completed(futures):
                        processed_chunk = future.result()
                        batch_results.append(processed_chunk)
                    
                    # Ghi t·∫•t c·∫£ k·∫øt qu·∫£ c·ªßa batch c√πng l√∫c
                    if batch_results:
                        combined_batch = pd.concat(batch_results, ignore_index=True)
                        
                        if first_chunk:
                            combined_batch.to_csv(output_file, index=False, mode='w')
                            first_chunk = False
                        else:
                            combined_batch.to_csv(output_file, index=False, mode='a', header=False)
                        
                        total_processed += len(combined_batch)
                    
                    # D·ªçn d·∫πp batch
                    chunk_batch = []
                    del batch_results
                    gc.collect()
                    
                    # Progress
                    elapsed = time.time() - start_time
                    speed = total_processed / elapsed if elapsed > 0 else 0
                    print(f"‚ö° {total_processed:,} d√≤ng | {speed:,.0f} d√≤ng/s")
        
        # X·ª≠ l√Ω batch cu·ªëi
        if chunk_batch:
            with ProcessPoolExecutor(max_workers=n_workers) as executor:
                futures = [executor.submit(fast_impute_chunk, chunk_data) for chunk_data in chunk_batch]
                
                for future in as_completed(futures):
                    processed_chunk = future.result()
                    
                    if first_chunk:
                        processed_chunk.to_csv(output_file, index=False, mode='w')
                        first_chunk = False
                    else:
                        processed_chunk.to_csv(output_file, index=False, mode='a', header=False)
                    
                    total_processed += len(processed_chunk)
    
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        return False
    
    elapsed_total = time.time() - start_time
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {total_processed:,} d√≤ng trong {elapsed_total:.1f} gi√¢y")
    print(f"‚ö° T·ªëc ƒë·ªô trung b√¨nh: {total_processed/elapsed_total:,.0f} d√≤ng/gi√¢y")
    print(f"üìÅ File k·∫øt qu·∫£: {output_file}")
    return True

def fast_impute_chunk(chunk):
    """
    N·ªôi suy nhanh cho m·ªôt chunk - T·ªëi ∆∞u t·ªëc ƒë·ªô cao
    """
    # L·∫•y c·ªôt s·ªë nhanh
    numeric_cols = chunk.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) == 0:
        return chunk.copy()
    
    # Ki·ªÉm tra missing nhanh
    missing_cols = [col for col in numeric_cols if chunk[col].isnull().any()]
    
    if len(missing_cols) == 0:
        return chunk.copy()
    
    # Chi·∫øn l∆∞·ª£c n·ªôi suy t·ªëi ∆∞u:
    for col in missing_cols:
        missing_pct = chunk[col].isnull().sum() / len(chunk) * 100
        
        if missing_pct > 70:
            # Qu√° nhi·ªÅu thi·∫øu -> median nhanh
            chunk[col].fillna(chunk[col].median(), inplace=True)
        
        elif missing_pct > 30:
            # Thi·∫øu nhi·ªÅu -> KNN nhanh (k=3)
            if len(numeric_cols) > 1:
                try:
                    imputer = KNNImputer(n_neighbors=3)
                    chunk.loc[:, numeric_cols] = imputer.fit_transform(chunk[numeric_cols])
                    break  # ƒê√£ impute t·∫•t c·∫£ c·ªôt s·ªë
                except:
                    chunk[col].fillna(chunk[col].median(), inplace=True)
            else:
                chunk[col].fillna(chunk[col].median(), inplace=True)
        
        else:
            # Thi·∫øu √≠t -> KNN ch√≠nh x√°c (k=5)
            if len(numeric_cols) > 3:
                try:
                    imputer = KNNImputer(n_neighbors=5)
                    chunk.loc[:, numeric_cols] = imputer.fit_transform(chunk[numeric_cols])
                    break  # ƒê√£ impute t·∫•t c·∫£ c·ªôt s·ªë
                except:
                    chunk[col].fillna(chunk[col].median(), inplace=True)
            else:
                chunk[col].fillna(chunk[col].median(), inplace=True)
    
    return chunk.copy()

def show_sample_output(output_file, num_rows=10):
    """
    Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u t·ª´ file k·∫øt qu·∫£
    """
    print(f"\nüìã === {num_rows} D√íNG ƒê·∫¶U C·ª¶A FILE {output_file} ===")
    
    try:
        # ƒê·ªçc file k·∫øt qu·∫£
        sample = pd.read_csv(output_file, nrows=num_rows)
        
        print(f"üìä K√≠ch th∆∞·ªõc file: {sample.shape[0]} d√≤ng (m·∫´u) √ó {sample.shape[1]} c·ªôt")
        print(f"üìã C√°c c·ªôt: {list(sample.columns)}")
        
        # Set display options for better formatting
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.float_format', '{:.6f}'.format)
        
        print(f"\nüìä D·ªÆ LI·ªÜU {num_rows} D√íNG ƒê·∫¶U:")
        print(sample.to_string(index=True))
        
        # Th·ªëng k√™ missing values sau n·ªôi suy
        print(f"\nüìà TH·ªêNG K√ä SAU N·ªòI SUY:")
        missing_info = sample.isnull().sum()
        missing_cols = missing_info[missing_info > 0]
        
        if len(missing_cols) > 0:
            print("‚ö†Ô∏è C√°c c·ªôt v·∫´n c√≤n missing:")
            for col, count in missing_cols.items():
                pct = (count / len(sample)) * 100
                print(f"  {col}: {count} ({pct:.1f}%)")
        else:
            print("‚úÖ Kh√¥ng c√≤n missing values!")
        
        # Th·ªëng k√™ c·ªôt s·ªë
        numeric_cols = sample.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\nüìä TH·ªêNG K√ä C√ÅC C·ªòT S·ªê:")
            for col in numeric_cols[:3]:  # Hi·ªÉn th·ªã 3 c·ªôt ƒë·∫ßu
                col_stats = sample[col].describe()
                print(f"  {col}: Min={col_stats['min']:.3f}, Max={col_stats['max']:.3f}, Mean={col_stats['mean']:.3f}")
        
        # Ki·ªÉm tra file size
        file_size = os.path.getsize(output_file) / (1024**3)  # GB
        print(f"\nüíæ K√≠ch th∆∞·ªõc file: {file_size:.2f} GB")
        
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")

def quick_twi_fix(input_file, output_file, chunk_size=1000000, n_workers=20):
    """
    S·ª≠a nhanh c·ªôt TWI b·ªã thi·∫øu v·ªõi multiprocessing - ƒê·∫∑c bi·ªát t·ªëi ∆∞u
    """
    print("üåä === S·ª¨A NHANH C·ªòT TWI V·ªöI MULTIPROCESSING ===")
    print(f"üíª C·∫•u h√¨nh: {n_workers} workers, chunk size {chunk_size:,}")
    
    start_time = time.time()
    first_chunk = True
    total_processed = 0
    batch_size = n_workers * 2
    chunk_batch = []
    
    chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, engine='c', buffer_lines=50000)
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        for i, chunk in enumerate(chunk_reader):
            chunk_batch.append(chunk)
            
            # X·ª≠ l√Ω batch khi ƒë·∫ßy
            if len(chunk_batch) >= batch_size:
                futures = [executor.submit(process_twi_chunk, chunk_data) for chunk_data in chunk_batch]
                
                batch_results = []
                for future in as_completed(futures):
                    processed_chunk = future.result()
                    batch_results.append(processed_chunk)
                
                # Ghi batch
                if batch_results:
                    combined_batch = pd.concat(batch_results, ignore_index=True)
                    
                    if first_chunk:
                        combined_batch.to_csv(output_file, index=False, mode='w')
                        first_chunk = False
                    else:
                        combined_batch.to_csv(output_file, index=False, mode='a', header=False)
                    
                    total_processed += len(combined_batch)
                
                # D·ªçn d·∫πp
                chunk_batch = []
                del batch_results
                gc.collect()
                
                # Progress
                elapsed = time.time() - start_time
                speed = total_processed / elapsed if elapsed > 0 else 0
                print(f"‚ö° TWI: {total_processed:,} d√≤ng | {speed:,.0f} d√≤ng/s")
    
    # X·ª≠ l√Ω batch cu·ªëi
    if chunk_batch:
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = [executor.submit(process_twi_chunk, chunk_data) for chunk_data in chunk_batch]
            
            for future in as_completed(futures):
                processed_chunk = future.result()
                
                if first_chunk:
                    processed_chunk.to_csv(output_file, index=False, mode='w')
                    first_chunk = False
                else:
                    processed_chunk.to_csv(output_file, index=False, mode='a', header=False)
                
                total_processed += len(processed_chunk)
    
    elapsed_total = time.time() - start_time
    print(f"\nüéâ Ho√†n th√†nh TWI! {total_processed:,} d√≤ng trong {elapsed_total:.1f} gi√¢y")
    print(f"‚ö° T·ªëc ƒë·ªô: {total_processed/elapsed_total:,.0f} d√≤ng/gi√¢y")

def process_twi_chunk(chunk):
    """
    Worker function x·ª≠ l√Ω TWI cho m·ªôt chunk
    """
    # X·ª≠ l√Ω TWI
    if 'twi' in chunk.columns:
        missing_pct = chunk['twi'].isnull().sum() / len(chunk) * 100
        
        if missing_pct > 0:
            # T√¨m c·ªôt ƒë·ªãa h√¨nh li√™n quan
            topo_cols = [col for col in ['dem', 'slope', 'curvature', 'aspect'] 
                       if col in chunk.columns and chunk[col].isnull().sum() < len(chunk) * 0.1]
            
            if len(topo_cols) >= 2 and missing_pct < 50:
                # D√πng RF nhanh
                mask_missing = chunk['twi'].isnull()
                chunk_complete = chunk[~mask_missing]
                chunk_missing = chunk[mask_missing]
                
                if len(chunk_complete) > 20:
                    try:
                        rf = RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=2)
                        rf.fit(chunk_complete[topo_cols], chunk_complete['twi'])
                        predicted = rf.predict(chunk_missing[topo_cols])
                        chunk.loc[mask_missing, 'twi'] = predicted
                    except:
                        chunk['twi'].fillna(chunk['twi'].median(), inplace=True)
                else:
                    chunk['twi'].fillna(chunk['twi'].median(), inplace=True)
            else:
                # D√πng median
                chunk['twi'].fillna(chunk['twi'].median(), inplace=True)
    
    return chunk.copy()

# S·ª≠ d·ª•ng
if __name__ == "__main__":
    # C·∫•u h√¨nh cho m√°y m·∫°nh
    INPUT_FILE = "your_large_file.csv"
    OUTPUT_FILE = "data_imputed.csv"
    CHUNK_SIZE = 1000000  # 1M d√≤ng cho m√°y m·∫°nh
    N_WORKERS = 20  # 20/24 cores
    
    print("üöÄ === CH∆Ø∆†NG TR√åNH N·ªòI SUY T·ªêI ∆ØU CAO ===")
    print(f"üíª C·∫•u h√¨nh: {N_WORKERS} workers, chunk {CHUNK_SIZE:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # Ch·ªçn m·ªôt trong hai:
    
    # 1. N·ªôi suy t·ªïng qu√°t (multiprocessing)
    success = quick_impute_large_csv(INPUT_FILE, OUTPUT_FILE, 
                                   chunk_size=CHUNK_SIZE, n_workers=N_WORKERS)
    
    # 2. Ch·ªâ s·ª≠a TWI (multiprocessing si√™u nhanh)
    # success = quick_twi_fix(INPUT_FILE, OUTPUT_FILE, 
    #                        chunk_size=CHUNK_SIZE, n_workers=N_WORKERS)
    
    if success:
        print("‚úÖ N·ªôi suy ho√†n th√†nh th√†nh c√¥ng!")
        
        # Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£
        show_sample_output(OUTPUT_FILE, num_rows=10)
        
        print(f"\nüéØ File k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u: {OUTPUT_FILE}")
        print("üëã Ch∆∞∆°ng tr√¨nh ho√†n th√†nh!")
    else:
        print("‚ùå C√≥ l·ªói x·∫£y ra!")