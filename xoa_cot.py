import pandas as pd
import numpy as np
import re
from tqdm import tqdm
import gc
import os
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import psutil
import warnings
warnings.filterwarnings('ignore')

def process_large_csv(input_file, output_file, chunk_size=1000000, n_workers=None):
    """
    Xá»­ lÃ½ file CSV lá»›n theo chunks vá»›i multiprocessing - Tá»‘i Æ°u tá»‘c Ä‘á»™ cao
    
    Args:
        input_file (str): ÄÆ°á»ng dáº«n file CSV Ä‘áº§u vÃ o
        output_file (str): ÄÆ°á»ng dáº«n file CSV Ä‘áº§u ra
        chunk_size (int): KÃ­ch thÆ°á»›c má»—i chunk (máº·c Ä‘á»‹nh 500K cho mÃ¡y máº¡nh)
        n_workers (int): Sá»‘ worker processes (None = auto detect)
    """
    
    # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ worker tá»‘i Æ°u
    if n_workers is None:
        n_workers = min(20, cpu_count() - 4)  # Äá»ƒ láº¡i 4 cores cho há»‡ thá»‘ng
    
    print(f"ğŸš€ Tá»‘i Æ°u tá»‘c Ä‘á»™: {n_workers} workers Ã— chunk {chunk_size:,}")
    print(f"ğŸ’¾ RAM kháº£ dá»¥ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # Kiá»ƒm tra file Ä‘áº§u vÃ o cÃ³ tá»“n táº¡i khÃ´ng
    if not os.path.exists(input_file):
        print(f"Lá»—i: File '{input_file}' khÃ´ng tá»“n táº¡i!")
        return False
    
    # Danh sÃ¡ch cÃ¡c cá»™t cáº§n giá»¯ láº¡i (khÃ´ng bao gá»“m flood)
    required_columns = [
        'lulc', 'Density_River', 'Density_Road', 
        'Distan2river_met', 'Distan2road_met', 'aspect', 
        'curvature', 'dem', 'flowDir', 'slope', 'twi', 'NDVI'
    ]
    
    # Äá»c header Ä‘á»ƒ phÃ¢n tÃ­ch cÃ¡c cá»™t
    print("Äang Ä‘á»c header vÃ  phÃ¢n tÃ­ch cáº¥u trÃºc dá»¯ liá»‡u...")
    sample_df = pd.read_csv(input_file, nrows=0)  # Chá»‰ Ä‘á»c header
    all_columns = sample_df.columns.tolist()
    
    print(f"Tá»•ng sá»‘ cá»™t trong file: {len(all_columns)}")
    
    # TÃ¬m cÃ¡c cá»™t tá»a Ä‘á»™ (lat, lon)
    coord_columns = []
    for col in all_columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['lat', 'lon', 'longitude', 'latitude', 'x', 'y']):
            coord_columns.append(col)
    
    print(f"CÃ¡c cá»™t tá»a Ä‘á»™ tÃ¬m tháº¥y: {coord_columns}")
    
    # TÃ¬m cá»™t NDVI 2024
    ndvi_2024_column = None
    for col in all_columns:
        col_lower = col.lower()
        if 'ndvi' in col_lower and '2024' in col:
            ndvi_2024_column = col
            break
    
    if ndvi_2024_column:
        print(f"Cá»™t NDVI 2024 tÃ¬m tháº¥y: {ndvi_2024_column}")
    else:
        print("KhÃ´ng tÃ¬m tháº¥y cá»™t NDVI 2024, sáº½ tÃ¬m cá»™t NDVI gáº§n nháº¥t")
        # TÃ¬m cá»™t NDVI gáº§n nháº¥t
        ndvi_columns = [col for col in all_columns if 'ndvi' in col.lower()]
        if ndvi_columns:
            ndvi_2024_column = ndvi_columns[-1]  # Láº¥y cá»™t cuá»‘i cÃ¹ng
            print(f"Sá»­ dá»¥ng cá»™t NDVI: {ndvi_2024_column}")
    
    # XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t cáº§n Ä‘á»c
    columns_to_read = []
    
    # ThÃªm cá»™t tá»a Ä‘á»™
    columns_to_read.extend(coord_columns)
    
    # ThÃªm cÃ¡c cá»™t cáº§n thiáº¿t
    for col in required_columns:
        if col == 'NDVI':
            if ndvi_2024_column and ndvi_2024_column in all_columns:
                columns_to_read.append(ndvi_2024_column)
        else:
            if col in all_columns:
                columns_to_read.append(col)
            else:
                print(f"Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y cá»™t '{col}'")
    
    # Loáº¡i bá» cá»™t trÃ¹ng láº·p
    columns_to_read = list(dict.fromkeys(columns_to_read))
    
    print(f"Tá»•ng sá»‘ cá»™t sáº½ Ä‘Æ°á»£c xá»­ lÃ½: {len(columns_to_read)}")
    
    # Xá»­ lÃ½ file theo chunks
    first_chunk = True
    total_rows_processed = 0
    
    print("Báº¯t Ä‘áº§u xá»­ lÃ½ file theo chunks...")
    
    try:
        # Äáº¿m tá»•ng sá»‘ dÃ²ng Ä‘á»ƒ hiá»ƒn thá»‹ progress
        print("Äang Ä‘áº¿m tá»•ng sá»‘ dÃ²ng...")
        total_lines = sum(1 for line in open(input_file, 'r', encoding='utf-8')) - 1  # Trá»« header
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        print(f"Tá»•ng sá»‘ dÃ²ng: {total_lines}, sá»‘ chunks: {total_chunks}")
        
        chunk_reader = pd.read_csv(
            input_file, 
            chunksize=chunk_size,
            usecols=columns_to_read,
            low_memory=False,
            engine='c',  # Sá»­ dá»¥ng C engine Ä‘á»ƒ tÄƒng tá»‘c
            buffer_lines=50000  # Buffer lá»›n cho I/O nhanh hÆ¡n
        )
        
        # Thu tháº­p cÃ¡c chunk Ä‘á»ƒ xá»­ lÃ½ batch
        print("ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ song song...")
        start_time = time.time()
        
        # Xá»­ lÃ½ theo batch lá»›n Ä‘á»ƒ táº­n dá»¥ng CPU
        batch_size = n_workers * 3  # TÄƒng batch size Ä‘á»ƒ giáº£m overhead
        chunk_batch = []
        processed_rows = 0
        
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            for i, chunk in enumerate(tqdm(chunk_reader, total=total_chunks, desc="ğŸ”„ Xá»­ lÃ½")):
                # Chuáº©n bá»‹ dá»¯ liá»‡u cho multiprocessing
                chunk_args = (chunk, ndvi_2024_column, coord_columns)
                chunk_batch.append(chunk_args)
                
                # Xá»­ lÃ½ batch khi Ä‘áº§y hoáº·c káº¿t thÃºc
                if len(chunk_batch) >= batch_size or i == total_chunks - 1:
                    # Submit batch Ä‘á»ƒ xá»­ lÃ½ song song
                    futures = [executor.submit(process_chunk_parallel, args) for args in chunk_batch]
                    
                    # Thu tháº­p káº¿t quáº£ vÃ  ghi file
                    batch_results = []
                    for future in as_completed(futures):
                        processed_chunk = future.result()
                        if not processed_chunk.empty:
                            batch_results.append(processed_chunk)
                    
                    # Ghi táº¥t cáº£ káº¿t quáº£ cá»§a batch cÃ¹ng lÃºc
                    if batch_results:
                        combined_batch = pd.concat(batch_results, ignore_index=True)
                        
                        if processed_rows == 0:
                            combined_batch.to_csv(output_file, index=False, mode='w')
                        else:
                            combined_batch.to_csv(output_file, index=False, mode='a', header=False)
                        
                        processed_rows += len(combined_batch)
                    
                    # Dá»n dáº¹p batch
                    chunk_batch = []
                    del batch_results
                    gc.collect()
                    
                    # Hiá»ƒn thá»‹ tiáº¿n Ä‘á»™ má»—i 10 batch Ä‘á»ƒ giáº£m overhead
                    if i % (batch_size * 10) == 0:
                        elapsed = time.time() - start_time
                        speed = processed_rows / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_chunks - i - 1) * chunk_size / speed if speed > 0 else 0
                        print(f"âš¡ {processed_rows:,} dÃ²ng | {speed:,.0f} dÃ²ng/s | ETA: {eta_seconds/60:.1f}m")
        
        total_rows_processed = processed_rows
    
    except Exception as e:
        print(f"Lá»—i khi xá»­ lÃ½ file: {str(e)}")
        return False
    
    print(f"HoÃ n thÃ nh! ÄÃ£ xá»­ lÃ½ {total_rows_processed:,} dÃ²ng")
    print(f"File káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_file}")
    
    # Hiá»ƒn thá»‹ thÃ´ng tin file káº¿t quáº£
    result_size = os.path.getsize(output_file) / (1024**3)  # GB
    print(f"KÃ­ch thÆ°á»›c file káº¿t quáº£: {result_size:.2f} GB")
    
    return True

def process_chunk_parallel(args):
    """
    Wrapper function Ä‘á»ƒ xá»­ lÃ½ chunk trong multiprocessing
    """
    chunk_data, ndvi_2024_column, coord_columns = args
    return process_chunk(chunk_data, [], ndvi_2024_column, coord_columns)

def process_chunk(chunk, flood_columns, ndvi_2024_column, coord_columns):
    """
    Xá»­ lÃ½ má»™t chunk dá»¯ liá»‡u - Tá»‘i Æ°u tá»‘c Ä‘á»™ cao
    
    Args:
        chunk (DataFrame): Chunk dá»¯ liá»‡u cáº§n xá»­ lÃ½
        flood_columns (list): Danh sÃ¡ch cÃ¡c cá»™t lá»‹ch sá»­ lÅ© (khÃ´ng sá»­ dá»¥ng)
        ndvi_2024_column (str): TÃªn cá»™t NDVI 2024
        coord_columns (list): Danh sÃ¡ch cÃ¡c cá»™t tá»a Ä‘á»™
    
    Returns:
        DataFrame: Chunk Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
    """
    
    # Äá»•i tÃªn cá»™t NDVI 2024 thÃ nh NDVI nhanh chÃ³ng
    if ndvi_2024_column and ndvi_2024_column in chunk.columns:
        chunk.rename(columns={ndvi_2024_column: 'NDVI'}, inplace=True)
    
    # Äá»‹nh nghÄ©a cá»™t má»¥c tiÃªu má»™t láº§n
    target_columns = coord_columns + [
        'lulc', 'Density_River', 'Density_Road', 
        'Distan2river_met', 'Distan2road_met', 'aspect', 
        'curvature', 'dem', 'flowDir', 'slope', 'twi', 'NDVI'
    ]
    
    # Lá»c cá»™t nhanh vá»›i intersection
    available_columns = [col for col in target_columns if col in chunk.columns]
    
    # Tráº£ vá» chunk Ä‘Ã£ lá»c
    return chunk[available_columns].copy()

def analyze_csv_structure(input_file, num_rows=1000):
    """
    PhÃ¢n tÃ­ch cáº¥u trÃºc file CSV Ä‘á»ƒ hiá»ƒu dá»¯ liá»‡u
    
    Args:
        input_file (str): ÄÆ°á»ng dáº«n file CSV
        num_rows (int): Sá»‘ dÃ²ng Ä‘á»ƒ phÃ¢n tÃ­ch máº«u
    """
    print("=== PHÃ‚N TÃCH Cáº¤U TRÃšC FILE CSV ===")
    
    # Äá»c máº«u dá»¯ liá»‡u
    sample_df = pd.read_csv(input_file, nrows=num_rows)
    
    print(f"KÃ­ch thÆ°á»›c máº«u: {sample_df.shape}")
    print(f"CÃ¡c cá»™t trong file ({len(sample_df.columns)}):")
    
    # PhÃ¢n loáº¡i cÃ¡c cá»™t
    coord_cols = []
    flood_cols = []
    ndvi_cols = []
    other_cols = []
    
    for col in sample_df.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['lat', 'lon', 'longitude', 'latitude', 'x', 'y']):
            coord_cols.append(col)
        elif 'flood' in col_lower:
            flood_cols.append(col)
        elif 'ndvi' in col_lower:
            ndvi_cols.append(col)
        else:
            other_cols.append(col)
    
    print(f"\nCÃ¡c cá»™t tá»a Ä‘á»™ ({len(coord_cols)}): {coord_cols}")
    print(f"CÃ¡c cá»™t lÅ© ({len(flood_cols)}): {flood_cols[:10]}{'...' if len(flood_cols) > 10 else ''}")
    print(f"CÃ¡c cá»™t NDVI ({len(ndvi_cols)}): {ndvi_cols[:10]}{'...' if len(ndvi_cols) > 10 else ''}")
    print(f"CÃ¡c cá»™t khÃ¡c ({len(other_cols)}): {other_cols[:10]}{'...' if len(other_cols) > 10 else ''}")
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª dá»¯ liá»‡u máº«u
    print(f"\nThá»‘ng kÃª máº«u dá»¯ liá»‡u:")
    print(sample_df.describe())
    
    return sample_df

def show_sample_output(output_file, num_rows=10):
    """
    Hiá»ƒn thá»‹ dá»¯ liá»‡u máº«u tá»« file káº¿t quáº£
    """
    print(f"\nğŸ“‹ === {num_rows} DÃ’NG Äáº¦U Cá»¦A FILE {output_file} ===")
    
    try:
        # Äá»c file káº¿t quáº£
        sample = pd.read_csv(output_file, nrows=num_rows)
        
        print(f"ğŸ“Š KÃ­ch thÆ°á»›c file: {sample.shape[0]} dÃ²ng (máº«u) Ã— {sample.shape[1]} cá»™t")
        print(f"ğŸ“‹ CÃ¡c cá»™t: {list(sample.columns)}")
        
        # Set display options for better formatting
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.float_format', '{:.6f}'.format)
        
        print(f"\nğŸ“Š Dá»® LIá»†U {num_rows} DÃ’NG Äáº¦U:")
        print(sample.to_string(index=True))
        
        # Thá»‘ng kÃª cÆ¡ báº£n
        print(f"\nğŸ“ˆ THá»NG KÃŠ CÆ  Báº¢N:")
        numeric_cols = sample.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            for col in numeric_cols[:3]:  # Hiá»ƒn thá»‹ 3 cá»™t sá»‘ Ä‘áº§u
                col_stats = sample[col].describe()
                print(f"  {col}: Min={col_stats['min']:.3f}, Max={col_stats['max']:.3f}, Mean={col_stats['mean']:.3f}")
        
        # Kiá»ƒm tra file size
        file_size = os.path.getsize(output_file) / (1024**3)  # GB
        print(f"\nğŸ’¾ KÃ­ch thÆ°á»›c file: {file_size:.2f} GB")
        
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file: {e}")

# CÃ¡ch sá»­ dá»¥ng
if __name__ == "__main__":
    # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n file cá»§a báº¡n
    INPUT_FILE = "your_large_file.csv"  # Thay báº±ng Ä‘Æ°á»ng dáº«n file 73GB cá»§a báº¡n
    OUTPUT_FILE = "processed_flood_data.csv"  # File káº¿t quáº£
    
    # Tá»‘i Æ°u cho mÃ¡y máº¡nh: 24 cores, 29GB RAM trá»‘ng
    CHUNK_SIZE = 1000000  # TÄƒng lÃªn 1M dÃ²ng/chunk Ä‘á»ƒ giáº£m overhead (tá»« 500K)
    N_WORKERS = 20  # Sá»­ dá»¥ng 20/24 cores, Ä‘á»ƒ láº¡i 4 cores cho há»‡ thá»‘ng
    
    print("ğŸš€ === Xá»¬ LÃ FILE CSV 73GB Vá»šI MULTIPROCESSING ===")
    print(f"ğŸ’» Cáº¥u hÃ¬nh: {N_WORKERS} workers, chunk size {CHUNK_SIZE:,}")
    print(f"ğŸ’¾ RAM kháº£ dá»¥ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # PhÃ¢n tÃ­ch cáº¥u trÃºc file trÆ°á»›c (tÃ¹y chá»n)
    try:
        analyze_csv_structure(INPUT_FILE, num_rows=1000)
    except Exception as e:
        print(f"KhÃ´ng thá»ƒ phÃ¢n tÃ­ch cáº¥u trÃºc: {e}")
    
    print("\n" + "="*60)
    
    # Xá»­ lÃ½ file chÃ­nh vá»›i multiprocessing
    start_total = time.time()
    success = process_large_csv(INPUT_FILE, OUTPUT_FILE, CHUNK_SIZE, N_WORKERS)
    end_total = time.time()
    
    if success:
        print(f"âœ… Xá»­ lÃ½ hoÃ n thÃ nh thÃ nh cÃ´ng trong {end_total - start_total:.1f} giÃ¢y!")
        
        # Hiá»ƒn thá»‹ 10 dÃ²ng Ä‘áº§u cá»§a file káº¿t quáº£
        show_sample_output(OUTPUT_FILE, num_rows=10)
        
        print(f"\nğŸ¯ File káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {OUTPUT_FILE}")
        print("ğŸ‘‹ ChÆ°Æ¡ng trÃ¬nh hoÃ n thÃ nh!")
    else:
        print("âŒ CÃ³ lá»—i xáº£y ra trong quÃ¡ trÃ¬nh xá»­ lÃ½!")