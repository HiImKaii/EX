import pandas as pd
import numpy as np
import re
from tqdm import tqdm
import gc
import os
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import psutil
import warnings
warnings.filterwarnings('ignore')

def process_large_csv(input_file, output_file, chunk_size=1000000, n_workers=None):
    """
    X·ª≠ l√Ω file CSV l·ªõn theo chunks v·ªõi multiprocessing - T·ªëi ∆∞u t·ªëc ƒë·ªô cao
    
    Args:
        input_file (str): ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu v√†o
        output_file (str): ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu ra
        chunk_size (int): K√≠ch th∆∞·ªõc m·ªói chunk (m·∫∑c ƒë·ªãnh 500K cho m√°y m·∫°nh)
        n_workers (int): S·ªë worker processes (None = auto detect)
    """
    
    # T·ª± ƒë·ªông ph√°t hi·ªán s·ªë worker t·ªëi ∆∞u
    if n_workers is None:
        n_workers = min(20, cpu_count() - 4)  # ƒê·ªÉ l·∫°i 4 cores cho h·ªá th·ªëng
    
    print(f"üöÄ T·ªëi ∆∞u t·ªëc ƒë·ªô: {n_workers} workers √ó chunk {chunk_size:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # Ki·ªÉm tra file ƒë·∫ßu v√†o c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(input_file):
        print(f"L·ªói: File '{input_file}' kh√¥ng t·ªìn t·∫°i!")
        return False
    
    # Danh s√°ch c√°c c·ªôt c·∫ßn gi·ªØ l·∫°i (kh√¥ng bao g·ªìm flood)
    required_columns = [
        'lulc', 'Density_River', 'Density_Road', 
        'Distan2river_met', 'Distan2road_met', 'aspect', 
        'curvature', 'dem', 'flowDir', 'slope', 'twi', 'NDVI'
    ]
    
    # ƒê·ªçc header ƒë·ªÉ ph√¢n t√≠ch c√°c c·ªôt
    print("ƒêang ƒë·ªçc header v√† ph√¢n t√≠ch c·∫•u tr√∫c d·ªØ li·ªáu...")
    sample_df = pd.read_csv(input_file, nrows=0)  # Ch·ªâ ƒë·ªçc header
    all_columns = sample_df.columns.tolist()
    
    print(f"T·ªïng s·ªë c·ªôt trong file: {len(all_columns)}")
    
    # T√¨m c√°c c·ªôt t·ªça ƒë·ªô (lat, lon)
    coord_columns = []
    for col in all_columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['lat', 'lon', 'longitude', 'latitude', 'x', 'y']):
            coord_columns.append(col)
    
    print(f"C√°c c·ªôt t·ªça ƒë·ªô t√¨m th·∫•y: {coord_columns}")
    
    # T√¨m c·ªôt NDVI 2024
    ndvi_2024_column = None
    for col in all_columns:
        col_lower = col.lower()
        if 'ndvi' in col_lower and '2024' in col:
            ndvi_2024_column = col
            break
    
    if ndvi_2024_column:
        print(f"C·ªôt NDVI 2024 t√¨m th·∫•y: {ndvi_2024_column}")
    else:
        print("Kh√¥ng t√¨m th·∫•y c·ªôt NDVI 2024, s·∫Ω t√¨m c·ªôt NDVI g·∫ßn nh·∫•t")
        # T√¨m c·ªôt NDVI g·∫ßn nh·∫•t
        ndvi_columns = [col for col in all_columns if 'ndvi' in col.lower()]
        if ndvi_columns:
            ndvi_2024_column = ndvi_columns[-1]  # L·∫•y c·ªôt cu·ªëi c√πng
            print(f"S·ª≠ d·ª•ng c·ªôt NDVI: {ndvi_2024_column}")
    
    # X√°c ƒë·ªãnh c√°c c·ªôt c·∫ßn ƒë·ªçc
    columns_to_read = []
    
    # Th√™m c·ªôt t·ªça ƒë·ªô
    columns_to_read.extend(coord_columns)
    
    # Th√™m c√°c c·ªôt c·∫ßn thi·∫øt
    for col in required_columns:
        if col == 'NDVI':
            if ndvi_2024_column and ndvi_2024_column in all_columns:
                columns_to_read.append(ndvi_2024_column)
        else:
            if col in all_columns:
                columns_to_read.append(col)
            else:
                print(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c·ªôt '{col}'")
    
    # Lo·∫°i b·ªè c·ªôt tr√πng l·∫∑p
    columns_to_read = list(dict.fromkeys(columns_to_read))
    
    print(f"T·ªïng s·ªë c·ªôt s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω: {len(columns_to_read)}")
    
    # X·ª≠ l√Ω file theo chunks
    first_chunk = True
    total_rows_processed = 0
    
    print("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file theo chunks...")
    
    try:
        # ƒê·∫øm t·ªïng s·ªë d√≤ng ƒë·ªÉ hi·ªÉn th·ªã progress
        print("ƒêang ƒë·∫øm t·ªïng s·ªë d√≤ng...")
        total_lines = sum(1 for line in open(input_file, 'r', encoding='utf-8')) - 1  # Tr·ª´ header
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        print(f"T·ªïng s·ªë d√≤ng: {total_lines}, s·ªë chunks: {total_chunks}")
        
        chunk_reader = pd.read_csv(
            input_file, 
            chunksize=chunk_size,
            usecols=columns_to_read,
            low_memory=False,
            engine='c',  # S·ª≠ d·ª•ng C engine ƒë·ªÉ tƒÉng t·ªëc
            buffer_lines=50000  # Buffer l·ªõn cho I/O nhanh h∆°n
        )
        
        # Thu th·∫≠p c√°c chunk ƒë·ªÉ x·ª≠ l√Ω batch
        print("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω song song...")
        start_time = time.time()
        
        # X·ª≠ l√Ω theo batch l·ªõn ƒë·ªÉ t·∫≠n d·ª•ng CPU
        batch_size = n_workers * 3  # TƒÉng batch size ƒë·ªÉ gi·∫£m overhead
        chunk_batch = []
        processed_rows = 0
        
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            for i, chunk in enumerate(tqdm(chunk_reader, total=total_chunks, desc="üîÑ X·ª≠ l√Ω")):
                # Chu·∫©n b·ªã d·ªØ li·ªáu cho multiprocessing
                chunk_args = (chunk, ndvi_2024_column, coord_columns)
                chunk_batch.append(chunk_args)
                
                # X·ª≠ l√Ω batch khi ƒë·∫ßy ho·∫∑c k·∫øt th√∫c
                if len(chunk_batch) >= batch_size or i == total_chunks - 1:
                    # Submit batch ƒë·ªÉ x·ª≠ l√Ω song song
                    futures = [executor.submit(process_chunk_parallel, args) for args in chunk_batch]
                    
                    # Thu th·∫≠p k·∫øt qu·∫£ v√† ghi file
                    batch_results = []
                    for future in as_completed(futures):
                        processed_chunk = future.result()
                        if not processed_chunk.empty:
                            batch_results.append(processed_chunk)
                    
                    # Ghi t·∫•t c·∫£ k·∫øt qu·∫£ c·ªßa batch c√πng l√∫c
                    if batch_results:
                        combined_batch = pd.concat(batch_results, ignore_index=True)
                        
                        if processed_rows == 0:
                            combined_batch.to_csv(output_file, index=False, mode='w')
                        else:
                            combined_batch.to_csv(output_file, index=False, mode='a', header=False)
                        
                        processed_rows += len(combined_batch)
                    
                    # D·ªçn d·∫πp batch
                    chunk_batch = []
                    del batch_results
                    gc.collect()
                    
                    # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô m·ªói 10 batch ƒë·ªÉ gi·∫£m overhead
                    if i % (batch_size * 10) == 0:
                        elapsed = time.time() - start_time
                        speed = processed_rows / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_chunks - i - 1) * chunk_size / speed if speed > 0 else 0
                        print(f"‚ö° {processed_rows:,} d√≤ng | {speed:,.0f} d√≤ng/s | ETA: {eta_seconds/60:.1f}m")
        
        total_rows_processed = processed_rows
    
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω file: {str(e)}")
        return False
    
    print(f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {total_rows_processed:,} d√≤ng")
    print(f"File k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_file}")
    
    # Hi·ªÉn th·ªã th√¥ng tin file k·∫øt qu·∫£
    result_size = os.path.getsize(output_file) / (1024**3)  # GB
    print(f"K√≠ch th∆∞·ªõc file k·∫øt qu·∫£: {result_size:.2f} GB")
    
    return True

def process_chunk_parallel(args):
    """
    Wrapper function ƒë·ªÉ x·ª≠ l√Ω chunk trong multiprocessing
    """
    chunk_data, ndvi_2024_column, coord_columns = args
    return process_chunk(chunk_data, [], ndvi_2024_column, coord_columns)

def process_chunk(chunk, flood_columns, ndvi_2024_column, coord_columns):
    """
    X·ª≠ l√Ω m·ªôt chunk d·ªØ li·ªáu - T·ªëi ∆∞u t·ªëc ƒë·ªô cao
    
    Args:
        chunk (DataFrame): Chunk d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω
        flood_columns (list): Danh s√°ch c√°c c·ªôt l·ªãch s·ª≠ l≈© (kh√¥ng s·ª≠ d·ª•ng)
        ndvi_2024_column (str): T√™n c·ªôt NDVI 2024
        coord_columns (list): Danh s√°ch c√°c c·ªôt t·ªça ƒë·ªô
    
    Returns:
        DataFrame: Chunk ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    
    # ƒê·ªïi t√™n c·ªôt NDVI 2024 th√†nh NDVI nhanh ch√≥ng
    if ndvi_2024_column and ndvi_2024_column in chunk.columns:
        chunk.rename(columns={ndvi_2024_column: 'NDVI'}, inplace=True)
    
    # ƒê·ªãnh nghƒ©a c·ªôt m·ª•c ti√™u m·ªôt l·∫ßn
    target_columns = coord_columns + [
        'lulc', 'Density_River', 'Density_Road', 
        'Distan2river_met', 'Distan2road_met', 'aspect', 
        'curvature', 'dem', 'flowDir', 'slope', 'twi', 'NDVI'
    ]
    
    # L·ªçc c·ªôt nhanh v·ªõi intersection
    available_columns = [col for col in target_columns if col in chunk.columns]
    
    # Tr·∫£ v·ªÅ chunk ƒë√£ l·ªçc
    return chunk[available_columns].copy()

def analyze_csv_structure(input_file, num_rows=1000):
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c file CSV ƒë·ªÉ hi·ªÉu d·ªØ li·ªáu
    
    Args:
        input_file (str): ƒê∆∞·ªùng d·∫´n file CSV
        num_rows (int): S·ªë d√≤ng ƒë·ªÉ ph√¢n t√≠ch m·∫´u
    """
    print("=== PH√ÇN T√çCH C·∫§U TR√öC FILE CSV ===")
    
    # ƒê·ªçc m·∫´u d·ªØ li·ªáu
    sample_df = pd.read_csv(input_file, nrows=num_rows)
    
    print(f"K√≠ch th∆∞·ªõc m·∫´u: {sample_df.shape}")
    print(f"C√°c c·ªôt trong file ({len(sample_df.columns)}):")
    
    # Ph√¢n lo·∫°i c√°c c·ªôt
    coord_cols = []
    flood_cols = []
    ndvi_cols = []
    other_cols = []
    
    for col in sample_df.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['lat', 'lon', 'longitude', 'latitude', 'x', 'y']):
            coord_cols.append(col)
        elif 'flood' in col_lower:
            flood_cols.append(col)
        elif 'ndvi' in col_lower:
            ndvi_cols.append(col)
        else:
            other_cols.append(col)
    
    print(f"\nC√°c c·ªôt t·ªça ƒë·ªô ({len(coord_cols)}): {coord_cols}")
    print(f"C√°c c·ªôt l≈© ({len(flood_cols)}): {flood_cols[:10]}{'...' if len(flood_cols) > 10 else ''}")
    print(f"C√°c c·ªôt NDVI ({len(ndvi_cols)}): {ndvi_cols[:10]}{'...' if len(ndvi_cols) > 10 else ''}")
    print(f"C√°c c·ªôt kh√°c ({len(other_cols)}): {other_cols[:10]}{'...' if len(other_cols) > 10 else ''}")
    
    # Hi·ªÉn th·ªã th·ªëng k√™ d·ªØ li·ªáu m·∫´u
    print(f"\nTh·ªëng k√™ m·∫´u d·ªØ li·ªáu:")
    print(sample_df.describe())
    
    return sample_df

def show_sample_output(output_file, num_rows=10):
    """
    Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u t·ª´ file k·∫øt qu·∫£
    """
    print(f"\nüìã === {num_rows} D√íNG ƒê·∫¶U C·ª¶A FILE {output_file} ===")
    
    try:
        # ƒê·ªçc file k·∫øt qu·∫£
        sample = pd.read_csv(output_file, nrows=num_rows)
        
        print(f"üìä K√≠ch th∆∞·ªõc file: {sample.shape[0]} d√≤ng (m·∫´u) √ó {sample.shape[1]} c·ªôt")
        print(f"üìã C√°c c·ªôt: {list(sample.columns)}")
        
        # Set display options for better formatting
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.float_format', '{:.6f}'.format)
        
        print(f"\nüìä D·ªÆ LI·ªÜU {num_rows} D√íNG ƒê·∫¶U:")
        print(sample.to_string(index=True))
        
        # Th·ªëng k√™ c∆° b·∫£n
        print(f"\nüìà TH·ªêNG K√ä C∆† B·∫¢N:")
        numeric_cols = sample.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            for col in numeric_cols[:3]:  # Hi·ªÉn th·ªã 3 c·ªôt s·ªë ƒë·∫ßu
                col_stats = sample[col].describe()
                print(f"  {col}: Min={col_stats['min']:.3f}, Max={col_stats['max']:.3f}, Mean={col_stats['mean']:.3f}")
        
        # Ki·ªÉm tra file size
        file_size = os.path.getsize(output_file) / (1024**3)  # GB
        print(f"\nüíæ K√≠ch th∆∞·ªõc file: {file_size:.2f} GB")
        
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")

# C√°ch s·ª≠ d·ª•ng
if __name__ == "__main__":
    # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n
    INPUT_FILE = "your_large_file.csv"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n file 73GB c·ªßa b·∫°n
    OUTPUT_FILE = "processed_flood_data.csv"  # File k·∫øt qu·∫£
    
    # T·ªëi ∆∞u cho m√°y m·∫°nh: 24 cores, 29GB RAM tr·ªëng
    CHUNK_SIZE = 1000000  # TƒÉng l√™n 1M d√≤ng/chunk ƒë·ªÉ gi·∫£m overhead (t·ª´ 500K)
    N_WORKERS = 20  # S·ª≠ d·ª•ng 20/24 cores, ƒë·ªÉ l·∫°i 4 cores cho h·ªá th·ªëng
    
    print("üöÄ === X·ª¨ L√ù FILE CSV 73GB V·ªöI MULTIPROCESSING ===")
    print(f"üíª C·∫•u h√¨nh: {N_WORKERS} workers, chunk size {CHUNK_SIZE:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # Ph√¢n t√≠ch c·∫•u tr√∫c file tr∆∞·ªõc (t√πy ch·ªçn)
    try:
        analyze_csv_structure(INPUT_FILE, num_rows=1000)
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch c·∫•u tr√∫c: {e}")
    
    print("\n" + "="*60)
    
    # X·ª≠ l√Ω file ch√≠nh v·ªõi multiprocessing
    start_total = time.time()
    success = process_large_csv(INPUT_FILE, OUTPUT_FILE, CHUNK_SIZE, N_WORKERS)
    end_total = time.time()
    
    if success:
        print(f"‚úÖ X·ª≠ l√Ω ho√†n th√†nh th√†nh c√¥ng trong {end_total - start_total:.1f} gi√¢y!")
        
        # Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£
        show_sample_output(OUTPUT_FILE, num_rows=10)
        
        print(f"\nüéØ File k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u: {OUTPUT_FILE}")
        print("üëã Ch∆∞∆°ng tr√¨nh ho√†n th√†nh!")
    else:
        print("‚ùå C√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω!")