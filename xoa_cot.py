import pandas as pd
import numpy as np
import re
from tqdm import tqdm
import gc
import os
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import psutil

def process_large_csv(input_file, output_file, chunk_size=100000, n_workers=None):
    """
    X·ª≠ l√Ω file CSV l·ªõn theo chunks v·ªõi multiprocessing
    
    Args:
        input_file (str): ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu v√†o
        output_file (str): ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu ra
        chunk_size (int): K√≠ch th∆∞·ªõc m·ªói chunk ƒë·ªÉ x·ª≠ l√Ω (tƒÉng l√™n cho m√°y m·∫°nh)
        n_workers (int): S·ªë worker processes (None = auto detect)
    """
    
    # T·ª± ƒë·ªông ph√°t hi·ªán s·ªë worker t·ªëi ∆∞u
    if n_workers is None:
        n_workers = min(24, cpu_count())  # T·ªëi ƒëa 24 cores nh∆∞ m√°y b·∫°n
    
    print(f"üöÄ S·ª≠ d·ª•ng {n_workers} workers v·ªõi chunk size {chunk_size:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # Ki·ªÉm tra file ƒë·∫ßu v√†o c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(input_file):
        print(f"L·ªói: File '{input_file}' kh√¥ng t·ªìn t·∫°i!")
        return False
    
    # Danh s√°ch c√°c c·ªôt c·∫ßn gi·ªØ l·∫°i (kh√¥ng bao g·ªìm flood)
    required_columns = [
        'lulc', 'Density_River', 'Density_Road', 
        'Distan2river_met', 'Distan2road_met', 'aspect', 
        'curvature', 'dem', 'flowDir', 'slope', 'twi', 'NDVI'
    ]
    
    # ƒê·ªçc header ƒë·ªÉ ph√¢n t√≠ch c√°c c·ªôt
    print("ƒêang ƒë·ªçc header v√† ph√¢n t√≠ch c·∫•u tr√∫c d·ªØ li·ªáu...")
    sample_df = pd.read_csv(input_file, nrows=0)  # Ch·ªâ ƒë·ªçc header
    all_columns = sample_df.columns.tolist()
    
    print(f"T·ªïng s·ªë c·ªôt trong file: {len(all_columns)}")
    
    # T√¨m c√°c c·ªôt t·ªça ƒë·ªô (lat, lon)
    coord_columns = []
    for col in all_columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['lat', 'lon', 'longitude', 'latitude', 'x', 'y']):
            coord_columns.append(col)
    
    print(f"C√°c c·ªôt t·ªça ƒë·ªô t√¨m th·∫•y: {coord_columns}")
    
    # T√¨m c·ªôt NDVI 2024
    ndvi_2024_column = None
    for col in all_columns:
        col_lower = col.lower()
        if 'ndvi' in col_lower and '2024' in col:
            ndvi_2024_column = col
            break
    
    if ndvi_2024_column:
        print(f"C·ªôt NDVI 2024 t√¨m th·∫•y: {ndvi_2024_column}")
    else:
        print("Kh√¥ng t√¨m th·∫•y c·ªôt NDVI 2024, s·∫Ω t√¨m c·ªôt NDVI g·∫ßn nh·∫•t")
        # T√¨m c·ªôt NDVI g·∫ßn nh·∫•t
        ndvi_columns = [col for col in all_columns if 'ndvi' in col.lower()]
        if ndvi_columns:
            ndvi_2024_column = ndvi_columns[-1]  # L·∫•y c·ªôt cu·ªëi c√πng
            print(f"S·ª≠ d·ª•ng c·ªôt NDVI: {ndvi_2024_column}")
    
    # X√°c ƒë·ªãnh c√°c c·ªôt c·∫ßn ƒë·ªçc
    columns_to_read = []
    
    # Th√™m c·ªôt t·ªça ƒë·ªô
    columns_to_read.extend(coord_columns)
    
    # Th√™m c√°c c·ªôt c·∫ßn thi·∫øt
    for col in required_columns:
        if col == 'NDVI':
            if ndvi_2024_column and ndvi_2024_column in all_columns:
                columns_to_read.append(ndvi_2024_column)
        else:
            if col in all_columns:
                columns_to_read.append(col)
            else:
                print(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c·ªôt '{col}'")
    
    # Lo·∫°i b·ªè c·ªôt tr√πng l·∫∑p
    columns_to_read = list(dict.fromkeys(columns_to_read))
    
    print(f"T·ªïng s·ªë c·ªôt s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω: {len(columns_to_read)}")
    
    # X·ª≠ l√Ω file theo chunks
    first_chunk = True
    total_rows_processed = 0
    
    print("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file theo chunks...")
    
    try:
        # ƒê·∫øm t·ªïng s·ªë d√≤ng ƒë·ªÉ hi·ªÉn th·ªã progress
        print("ƒêang ƒë·∫øm t·ªïng s·ªë d√≤ng...")
        total_lines = sum(1 for line in open(input_file, 'r', encoding='utf-8')) - 1  # Tr·ª´ header
        total_chunks = (total_lines + chunk_size - 1) // chunk_size
        print(f"T·ªïng s·ªë d√≤ng: {total_lines}, s·ªë chunks: {total_chunks}")
        
        chunk_reader = pd.read_csv(
            input_file, 
            chunksize=chunk_size,
            usecols=columns_to_read,
            low_memory=False
        )
        
        # Thu th·∫≠p c√°c chunk ƒë·ªÉ x·ª≠ l√Ω batch
        print("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω song song...")
        start_time = time.time()
        
        # X·ª≠ l√Ω theo batch ƒë·ªÉ t·∫≠n d·ª•ng RAM
        batch_size = max(1, min(n_workers * 2, 32))  # Batch size t·ªëi ∆∞u
        chunk_batch = []
        processed_rows = 0
        
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            for i, chunk in enumerate(tqdm(chunk_reader, total=total_chunks, desc="ƒê·ªçc chunks")):
                # Chu·∫©n b·ªã d·ªØ li·ªáu cho multiprocessing
                chunk_args = (chunk, ndvi_2024_column, coord_columns)
                chunk_batch.append(chunk_args)
                
                # X·ª≠ l√Ω batch khi ƒë·∫ßy ho·∫∑c k·∫øt th√∫c
                if len(chunk_batch) >= batch_size or i == total_chunks - 1:
                    # Submit batch ƒë·ªÉ x·ª≠ l√Ω song song
                    futures = [executor.submit(process_chunk_parallel, args) for args in chunk_batch]
                    
                    # Thu th·∫≠p k·∫øt qu·∫£ v√† ghi file
                    for j, future in enumerate(as_completed(futures)):
                        processed_chunk = future.result()
                        
                        if not processed_chunk.empty:
                            # Ghi chunk ƒë√£ x·ª≠ l√Ω
                            if processed_rows == 0:
                                processed_chunk.to_csv(output_file, index=False, mode='w')
                            else:
                                processed_chunk.to_csv(output_file, index=False, mode='a', header=False)
                            
                            processed_rows += len(processed_chunk)
                    
                    # D·ªçn d·∫πp batch
                    chunk_batch = []
                    gc.collect()
                    
                    # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
                    elapsed = time.time() - start_time
                    speed = processed_rows / elapsed if elapsed > 0 else 0
                    print(f"‚ö° ƒê√£ x·ª≠ l√Ω {processed_rows:,} d√≤ng - T·ªëc ƒë·ªô: {speed:,.0f} d√≤ng/gi√¢y")
        
        total_rows_processed = processed_rows
    
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω file: {str(e)}")
        return False
    
    print(f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {total_rows_processed:,} d√≤ng")
    print(f"File k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_file}")
    
    # Hi·ªÉn th·ªã th√¥ng tin file k·∫øt qu·∫£
    result_size = os.path.getsize(output_file) / (1024**3)  # GB
    print(f"K√≠ch th∆∞·ªõc file k·∫øt qu·∫£: {result_size:.2f} GB")
    
    return True

def process_chunk_parallel(args):
    """
    Wrapper function ƒë·ªÉ x·ª≠ l√Ω chunk trong multiprocessing
    """
    chunk_data, ndvi_2024_column, coord_columns = args
    return process_chunk(chunk_data, [], ndvi_2024_column, coord_columns)

def process_chunk(chunk, flood_columns, ndvi_2024_column, coord_columns):
    """
    X·ª≠ l√Ω m·ªôt chunk d·ªØ li·ªáu
    
    Args:
        chunk (DataFrame): Chunk d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω
        flood_columns (list): Danh s√°ch c√°c c·ªôt l·ªãch s·ª≠ l≈© (kh√¥ng s·ª≠ d·ª•ng)
        ndvi_2024_column (str): T√™n c·ªôt NDVI 2024
        coord_columns (list): Danh s√°ch c√°c c·ªôt t·ªça ƒë·ªô
    
    Returns:
        DataFrame: Chunk ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    
    # ƒê·ªïi t√™n c·ªôt NDVI 2024 th√†nh NDVI
    if ndvi_2024_column and ndvi_2024_column in chunk.columns:
        chunk = chunk.rename(columns={ndvi_2024_column: 'NDVI'})
    
    # S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt: lat, lon, lulc, r·ªìi c√°c feature kh√°c
    final_columns = coord_columns + [
        'lulc', 'Density_River', 'Density_Road', 
        'Distan2river_met', 'Distan2road_met', 'aspect', 
        'curvature', 'dem', 'flowDir', 'slope', 'twi', 'NDVI'
    ]
    
    # Ch·ªâ gi·ªØ c√°c c·ªôt c√≥ trong chunk
    available_columns = [col for col in final_columns if col in chunk.columns]
    chunk = chunk[available_columns]
    
    return chunk

def analyze_csv_structure(input_file, num_rows=1000):
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c file CSV ƒë·ªÉ hi·ªÉu d·ªØ li·ªáu
    
    Args:
        input_file (str): ƒê∆∞·ªùng d·∫´n file CSV
        num_rows (int): S·ªë d√≤ng ƒë·ªÉ ph√¢n t√≠ch m·∫´u
    """
    print("=== PH√ÇN T√çCH C·∫§U TR√öC FILE CSV ===")
    
    # ƒê·ªçc m·∫´u d·ªØ li·ªáu
    sample_df = pd.read_csv(input_file, nrows=num_rows)
    
    print(f"K√≠ch th∆∞·ªõc m·∫´u: {sample_df.shape}")
    print(f"C√°c c·ªôt trong file ({len(sample_df.columns)}):")
    
    # Ph√¢n lo·∫°i c√°c c·ªôt
    coord_cols = []
    flood_cols = []
    ndvi_cols = []
    other_cols = []
    
    for col in sample_df.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['lat', 'lon', 'longitude', 'latitude', 'x', 'y']):
            coord_cols.append(col)
        elif 'flood' in col_lower:
            flood_cols.append(col)
        elif 'ndvi' in col_lower:
            ndvi_cols.append(col)
        else:
            other_cols.append(col)
    
    print(f"\nC√°c c·ªôt t·ªça ƒë·ªô ({len(coord_cols)}): {coord_cols}")
    print(f"C√°c c·ªôt l≈© ({len(flood_cols)}): {flood_cols[:10]}{'...' if len(flood_cols) > 10 else ''}")
    print(f"C√°c c·ªôt NDVI ({len(ndvi_cols)}): {ndvi_cols[:10]}{'...' if len(ndvi_cols) > 10 else ''}")
    print(f"C√°c c·ªôt kh√°c ({len(other_cols)}): {other_cols[:10]}{'...' if len(other_cols) > 10 else ''}")
    
    # Hi·ªÉn th·ªã th·ªëng k√™ d·ªØ li·ªáu m·∫´u
    print(f"\nTh·ªëng k√™ m·∫´u d·ªØ li·ªáu:")
    print(sample_df.describe())
    
    return sample_df

# C√°ch s·ª≠ d·ª•ng
if __name__ == "__main__":
    # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n
    INPUT_FILE = "your_large_file.csv"  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n file 73GB c·ªßa b·∫°n
    OUTPUT_FILE = "processed_flood_data.csv"  # File k·∫øt qu·∫£
    
    # T·ªëi ∆∞u cho m√°y m·∫°nh: 24 cores, 29GB RAM tr·ªëng
    CHUNK_SIZE = 500000  # TƒÉng l√™n 500k d√≤ng m·ªói chunk (t·ª´ 10k)
    N_WORKERS = 20  # S·ª≠ d·ª•ng 20/24 cores, ƒë·ªÉ l·∫°i 4 cores cho h·ªá th·ªëng
    
    print("üöÄ === X·ª¨ L√ù FILE CSV 73GB V·ªöI MULTIPROCESSING ===")
    print(f"üíª C·∫•u h√¨nh: {N_WORKERS} workers, chunk size {CHUNK_SIZE:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # Ph√¢n t√≠ch c·∫•u tr√∫c file tr∆∞·ªõc (t√πy ch·ªçn)
    try:
        analyze_csv_structure(INPUT_FILE, num_rows=1000)
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch c·∫•u tr√∫c: {e}")
    
    print("\n" + "="*60)
    
    # X·ª≠ l√Ω file ch√≠nh v·ªõi multiprocessing
    start_total = time.time()
    success = process_large_csv(INPUT_FILE, OUTPUT_FILE, CHUNK_SIZE, N_WORKERS)
    end_total = time.time()
    
    if success:
        print(f"‚úÖ X·ª≠ l√Ω ho√†n th√†nh th√†nh c√¥ng trong {end_total - start_total:.1f} gi√¢y!")
        
        # Ki·ªÉm tra k·∫øt qu·∫£
    
    if success:
        print("‚úÖ X·ª≠ l√Ω ho√†n th√†nh th√†nh c√¥ng!")
        
        # Ki·ªÉm tra k·∫øt qu·∫£
        result_sample = pd.read_csv(OUTPUT_FILE, nrows=5)
        print(f"\nM·∫´u d·ªØ li·ªáu k·∫øt qu·∫£:")
        print(result_sample)
        print(f"C√°c c·ªôt trong file k·∫øt qu·∫£: {list(result_sample.columns)}")
    else:
        print("‚ùå C√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω!")