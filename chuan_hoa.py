import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import psutil
import os
import warnings
warnings.filterwarnings('ignore')

class CSVDataNormalizer:
    def __init__(self, file_path, chunk_size=1000000, n_workers=None):
        """
        Kh·ªüi t·∫°o v·ªõi ƒë∆∞·ªùng d·∫´n file CSV - T·ªëi ∆∞u t·ªëc ƒë·ªô cao
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file CSV
            chunk_size: K√≠ch th∆∞·ªõc chunk (1M cho m√°y m·∫°nh)
            n_workers: S·ªë workers (None = auto detect 20/24 cores)
        """
        self.file_path = file_path
        self.chunk_size = chunk_size
        self.n_workers = min(20, cpu_count() - 4) if n_workers is None else n_workers  # ƒê·ªÉ l·∫°i 4 cores
        self.df = None
        self.original_df = None
        self.flood_column = None
        self.processing_columns = []
        
        print(f"üöÄ Normalizer t·ªëi ∆∞u: {self.n_workers} workers, chunk {self.chunk_size:,}")
        print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
        
    def load_data(self):
        """T·∫£i d·ªØ li·ªáu t·ª´ file CSV"""
        try:
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            
            for encoding in encodings:
                try:
                    self.df = pd.read_csv(self.file_path, encoding=encoding)
                    self.original_df = self.df.copy()  # L∆∞u b·∫£n g·ªëc
                    print(f"‚úÖ ƒê·ªçc file th√†nh c√¥ng: {self.df.shape[0]} h√†ng, {self.df.shape[1]} c·ªôt")
                    break
                except UnicodeDecodeError:
                    continue
            
            if self.df is None:
                raise Exception("Kh√¥ng th·ªÉ ƒë·ªçc file v·ªõi b·∫•t k·ª≥ encoding n√†o")
            
            self._identify_columns()
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói: {str(e)}")
            return False
    
    def _identify_columns(self):
        """X√°c ƒë·ªãnh c√°c c·ªôt c·∫ßn x·ª≠ l√Ω"""
        columns = list(self.df.columns)
        print(f"\nüìã Danh s√°ch t·∫•t c·∫£ c√°c c·ªôt: {columns}")
        
        if len(columns) >= 1:
            self.flood_column = columns[0]
            self.processing_columns = columns[1:]  # T·∫•t c·∫£ c√°c c·ªôt tr·ª´ c·ªôt flood
        else:
            print("‚ö†Ô∏è File ph·∫£i c√≥ √≠t nh·∫•t 1 c·ªôt!")
            return
        
        print(f"üåä C·ªôt nh√£n l≈© (kh√¥ng x·ª≠ l√Ω): {self.flood_column}")
        print(f"üîß C√°c c·ªôt c·∫ßn x·ª≠ l√Ω ({len(self.processing_columns)}): {self.processing_columns}")
        
        # Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt v·ªÅ t·ª´ng c·ªôt
        print(f"\nüìä Th√¥ng tin chi ti·∫øt t·ª´ng c·ªôt:")
        for i, col in enumerate(self.processing_columns):
            dtype = self.df[col].dtype
            null_count = self.df[col].isnull().sum()
            unique_count = self.df[col].nunique()
            print(f"  {i+1:2d}. {col:20s} | Ki·ªÉu: {str(dtype):10s} | Null: {null_count:4d} | Unique: {unique_count:4d}")
    
    def show_column_statistics(self, col_name):
        """Hi·ªÉn th·ªã th·ªëng k√™ c·ªßa m·ªôt c·ªôt"""
        if col_name not in self.df.columns:
            print(f"‚ùå C·ªôt '{col_name}' kh√¥ng t·ªìn t·∫°i")
            return
        
        print(f"\nüìà Th·ªëng k√™ c·ªôt '{col_name}':")
        print(f"  Ki·ªÉu d·ªØ li·ªáu: {self.df[col_name].dtype}")
        print(f"  S·ªë gi√° tr·ªã null: {self.df[col_name].isnull().sum()}")
        print(f"  S·ªë gi√° tr·ªã unique: {self.df[col_name].nunique()}")
        
        if pd.api.types.is_numeric_dtype(self.df[col_name]):
            stats = self.df[col_name].describe()
            print(f"  Min: {stats['min']:.4f}")
            print(f"  Max: {stats['max']:.4f}")
            print(f"  Mean: {stats['mean']:.4f}")
            print(f"  Std: {stats['std']:.4f}")
        else:
            print(f"  Top values: {dict(self.df[col_name].value_counts().head(3))}")
    
    def handle_missing_values_for_column(self, col_name):
        """X·ª≠ l√Ω gi√° tr·ªã thi·∫øu cho m·ªôt c·ªôt c·ª• th·ªÉ"""
        if col_name not in self.processing_columns:
            print(f"‚ùå C·ªôt '{col_name}' kh√¥ng trong danh s√°ch x·ª≠ l√Ω")
            return False
        
        null_count = self.df[col_name].isnull().sum()
        if null_count == 0:
            print(f"‚úÖ C·ªôt '{col_name}': Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu")
            return True
        
        print(f"üîß X·ª≠ l√Ω {null_count} gi√° tr·ªã thi·∫øu trong c·ªôt '{col_name}'")
        
        if pd.api.types.is_numeric_dtype(self.df[col_name]):
            # C·ªôt s·ªë: ƒëi·ªÅn b·∫±ng mean
            mean_val = self.df[col_name].mean()
            self.df[col_name].fillna(mean_val, inplace=True)
            print(f"  ‚û§ ƒêi·ªÅn b·∫±ng mean: {mean_val:.4f}")
        else:
            # C·ªôt ph√¢n lo·∫°i: ƒëi·ªÅn b·∫±ng mode
            mode_val = self.df[col_name].mode()[0] if not self.df[col_name].mode().empty else 'Unknown'
            self.df[col_name].fillna(mode_val, inplace=True)
            print(f"  ‚û§ ƒêi·ªÅn b·∫±ng mode: '{mode_val}'")
        
        return True
    
    def normalize_numeric_column(self, col_name, method='minmax'):
        """Chu·∫©n h√≥a m·ªôt c·ªôt s·ªë v·ªÅ kho·∫£ng [0,1]"""
        if col_name not in self.processing_columns:
            print(f"‚ùå C·ªôt '{col_name}' kh√¥ng trong danh s√°ch x·ª≠ l√Ω")
            return False
        
        if not pd.api.types.is_numeric_dtype(self.df[col_name]):
            print(f"‚ö†Ô∏è C·ªôt '{col_name}' kh√¥ng ph·∫£i l√† s·ªë - b·ªè qua chu·∫©n h√≥a")
            return False
        
        original_stats = {
            'min': self.df[col_name].min(),
            'max': self.df[col_name].max(),
            'mean': self.df[col_name].mean(),
            'std': self.df[col_name].std()
        }
        
        print(f"üéØ Chu·∫©n h√≥a c·ªôt '{col_name}' b·∫±ng ph∆∞∆°ng ph√°p '{method}'")
        print(f"  Tr∆∞·ªõc: min={original_stats['min']:.4f}, max={original_stats['max']:.4f}")
        
        if method == 'minmax':
            # Min-Max: (x - min) / (max - min) -> [0, 1]
            min_val = self.df[col_name].min()
            max_val = self.df[col_name].max()
            if max_val != min_val:
                self.df[col_name] = (self.df[col_name] - min_val) / (max_val - min_val)
            else:
                print(f"  ‚ö†Ô∏è T·∫•t c·∫£ gi√° tr·ªã ƒë·ªÅu b·∫±ng nhau: {min_val}")
                self.df[col_name] = 0  # G√°n v·ªÅ 0 n·∫øu t·∫•t c·∫£ gi√° tr·ªã gi·ªëng nhau
                
        elif method == 'standard':
            # Z-score: (x - mean) / std, sau ƒë√≥ √°p d·ª•ng sigmoid ƒë·ªÉ ƒë∆∞a v·ªÅ [0,1]
            mean_val = self.df[col_name].mean()
            std_val = self.df[col_name].std()
            if std_val != 0:
                z_scores = (self.df[col_name] - mean_val) / std_val
                # √Åp d·ª•ng sigmoid: 1 / (1 + exp(-z))
                self.df[col_name] = 1 / (1 + np.exp(-z_scores))
            else:
                self.df[col_name] = 0.5  # G√°n v·ªÅ 0.5 n·∫øu std = 0
        
        new_stats = {
            'min': self.df[col_name].min(),
            'max': self.df[col_name].max(),
            'mean': self.df[col_name].mean()
        }
        
        print(f"  Sau:  min={new_stats['min']:.4f}, max={new_stats['max']:.4f}, mean={new_stats['mean']:.4f}")
        return True
    
    def encode_categorical_column(self, col_name):
        """M√£ h√≥a m·ªôt c·ªôt ph√¢n lo·∫°i th√†nh s·ªë trong kho·∫£ng [0,1]"""
        if col_name not in self.processing_columns:
            print(f"‚ùå C·ªôt '{col_name}' kh√¥ng trong danh s√°ch x·ª≠ l√Ω")
            return False
        
        if pd.api.types.is_numeric_dtype(self.df[col_name]):
            print(f"‚ö†Ô∏è C·ªôt '{col_name}' ƒë√£ l√† s·ªë - b·ªè qua m√£ h√≥a")
            return False
        
        print(f"üè∑Ô∏è M√£ h√≥a c·ªôt ph√¢n lo·∫°i '{col_name}'")
        unique_vals = self.df[col_name].nunique()
        print(f"  S·ªë l∆∞·ª£ng nh√≥m: {unique_vals}")
        
        # X·ª≠ l√Ω NaN tr∆∞·ªõc khi m√£ h√≥a
        if self.df[col_name].isnull().any():
            self.df[col_name] = self.df[col_name].fillna('__MISSING__')
        
        # M√£ h√≥a b·∫±ng LabelEncoder
        le = LabelEncoder()
        encoded_values = le.fit_transform(self.df[col_name])
        
        # Chu·∫©n h√≥a v·ªÅ [0,1] n·∫øu c√≥ nhi·ªÅu h∆°n 1 nh√≥m
        if len(le.classes_) > 1:
            encoded_values = encoded_values / (len(le.classes_) - 1)
        else:
            encoded_values = np.zeros_like(encoded_values)
        
        self.df[col_name] = encoded_values
        
        print(f"  √Ånh x·∫°: {dict(zip(le.classes_, np.unique(encoded_values)))}")
        return True
    
    def process_column(self, col_name, normalization_method='minmax'):
        """X·ª≠ l√Ω ho√†n ch·ªânh m·ªôt c·ªôt: missing values + chu·∫©n h√≥a/m√£ h√≥a"""
        print(f"\n{'='*60}")
        print(f"üîÑ X·ª¨ L√ù C·ªòT: '{col_name}'")
        print(f"{'='*60}")
        
        # Hi·ªÉn th·ªã th·ªëng k√™ ban ƒë·∫ßu
        self.show_column_statistics(col_name)
        
        # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
        if not self.handle_missing_values_for_column(col_name):
            return False
        
        # Chu·∫©n h√≥a ho·∫∑c m√£ h√≥a
        if pd.api.types.is_numeric_dtype(self.df[col_name]):
            success = self.normalize_numeric_column(col_name, normalization_method)
        else:
            success = self.encode_categorical_column(col_name)
        
        if success:
            print(f"‚úÖ Ho√†n th√†nh x·ª≠ l√Ω c·ªôt '{col_name}'")
        
        return success
    
    def is_column_normalized(self, col_name):
        """Ki·ªÉm tra xem m·ªôt c·ªôt ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a hay ch∆∞a"""
        if not pd.api.types.is_numeric_dtype(self.df[col_name]):
            return False
        
        min_val = self.df[col_name].min()
        max_val = self.df[col_name].max()
        
        # Ki·ªÉm tra xem gi√° tr·ªã c√≥ n·∫±m trong kho·∫£ng [0,1] kh√¥ng
        return (min_val >= -1e-10) and (max_val <= 1 + 1e-10)
    
    def process_all_columns(self, normalization_method='minmax'):
        """X·ª≠ l√Ω t·∫•t c·∫£ c√°c c·ªôt ch∆∞a ƒë∆∞·ª£c chu·∫©n h√≥a (tr·ª´ c·ªôt flood)"""
        print("üöÄ B·∫ÆT ƒê·∫¶U KI·ªÇM TRA V√Ä X·ª¨ L√ù C√ÅC C·ªòT")
        print("=" * 80)
        
        if not self.processing_columns:
            print("‚ö†Ô∏è Kh√¥ng c√≥ c·ªôt n√†o ƒë·ªÉ x·ª≠ l√Ω")
            return False
        
        columns_to_process = []
        for col in self.processing_columns:
            if not self.is_column_normalized(col):
                columns_to_process.append(col)
                
        if not columns_to_process:
            print("‚úÖ T·∫•t c·∫£ c√°c c·ªôt ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a")
            return True
            
        print(f"\nüéØ Ph√°t hi·ªán {len(columns_to_process)} c·ªôt c·∫ßn chu·∫©n h√≥a:")
        for col in columns_to_process:
            print(f"  - {col}")
        
        success_count = 0
        for i, col in enumerate(columns_to_process):
            print(f"\n[{i+1}/{len(columns_to_process)}] ƒêang x·ª≠ l√Ω c·ªôt: {col}")
            if self.process_column(col, normalization_method):
                success_count += 1
        
        print(f"\nüìä K·∫æT QU·∫¢ T·ªîNG QUAN:")
        print(f"  T·ªïng s·ªë c·ªôt c·∫ßn x·ª≠ l√Ω: {len(columns_to_process)}")
        print(f"  X·ª≠ l√Ω th√†nh c√¥ng: {success_count}")
        print(f"  X·ª≠ l√Ω th·∫•t b·∫°i: {len(columns_to_process) - success_count}")
        
        return success_count == len(columns_to_process)
    
    def save_data(self, suffix="_normalized"):
        """L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω"""
        base_name = os.path.splitext(self.file_path)[0]
        output_path = f"{base_name}{suffix}.csv"
        
        try:
            self.df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"üíæ ƒê√£ l∆∞u file: {output_path}")
            print(f"üìè K√≠ch th∆∞·ªõc: {self.df.shape[0]} h√†ng x {self.df.shape[1]} c·ªôt")
            return output_path
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u file: {str(e)}")
            return None
    
    def show_comparison(self, col_name, sample_size=10):
        """So s√°nh d·ªØ li·ªáu tr∆∞·ªõc v√† sau x·ª≠ l√Ω"""
        if col_name not in self.df.columns:
            print(f"‚ùå C·ªôt '{col_name}' kh√¥ng t·ªìn t·∫°i")
            return
        
        print(f"\nüîç SO S√ÅNH D·ªÆ LI·ªÜU C·ªòT '{col_name}':")
        comparison_df = pd.DataFrame({
            'Tr∆∞·ªõc': self.original_df[col_name].head(sample_size),
            'Sau': self.df[col_name].head(sample_size)
        })
        print(comparison_df)

def process_chunk_normalization(args):
    """
    Worker function ƒë·ªÉ chu·∫©n h√≥a chunk song song
    """
    chunk_data, processing_columns, flood_column, lulc_column, normalization_method = args
    
    # X·ª≠ l√Ω missing values
    for col in processing_columns:
        if col in chunk_data.columns and chunk_data[col].isnull().sum() > 0:
            if pd.api.types.is_numeric_dtype(chunk_data[col]):
                chunk_data[col].fillna(chunk_data[col].mean(), inplace=True)
            else:
                mode_val = chunk_data[col].mode()[0] if not chunk_data[col].mode().empty else 'Unknown'
                chunk_data[col].fillna(mode_val, inplace=True)
    
    # Chu·∫©n h√≥a c√°c c·ªôt s·ªë
    for col in processing_columns:
        if col in chunk_data.columns and pd.api.types.is_numeric_dtype(chunk_data[col]):
            if normalization_method == 'minmax':
                min_val = chunk_data[col].min()
                max_val = chunk_data[col].max()
                if max_val != min_val:
                    chunk_data[col] = (chunk_data[col] - min_val) / (max_val - min_val)
                else:
                    chunk_data[col] = 0
            elif normalization_method == 'standard':
                mean_val = chunk_data[col].mean()
                std_val = chunk_data[col].std()
                if std_val != 0:
                    z_scores = (chunk_data[col] - mean_val) / std_val
                    chunk_data[col] = 1 / (1 + np.exp(-z_scores))
                else:
                    chunk_data[col] = 0.5
    
    return chunk_data

def normalize_large_csv_parallel(input_file, output_file, chunk_size=1000000, n_workers=20, normalization_method='minmax'):
    """
    Chu·∫©n h√≥a file CSV l·ªõn v·ªõi multiprocessing - T·ªëi ∆∞u t·ªëc ƒë·ªô cao
    
    Args:
        input_file: File ƒë·∫ßu v√†o
        output_file: File ƒë·∫ßu ra
        chunk_size: K√≠ch th∆∞·ªõc chunk (1M cho m√°y m·∫°nh)
        n_workers: S·ªë workers (20 cho m√°y 24 cores)
        normalization_method: 'minmax' ho·∫∑c 'standard'
    """
    print(f"üöÄ === CHU·∫®N H√ìA FILE L·ªöN V·ªöI MULTIPROCESSING T·ªêI ∆ØU ===")
    print(f"üíª C·∫•u h√¨nh: {n_workers} workers, chunk size {chunk_size:,}")
    print(f"üíæ RAM kh·∫£ d·ª•ng: {psutil.virtual_memory().available / (1024**3):.1f}GB")
    
    # ƒê·ªçc sample ƒë·ªÉ x√°c ƒë·ªãnh c·∫•u tr√∫c
    print("üìä Ph√¢n t√≠ch c·∫•u tr√∫c file...")
    sample = pd.read_csv(input_file, nrows=10000)
    columns = list(sample.columns)
    
    if len(columns) < 2:
        print("‚ùå File ph·∫£i c√≥ √≠t nh·∫•t 2 c·ªôt!")
        return False
    
    flood_column = columns[0]
    lulc_column = columns[1] if len(columns) > 1 else None
    processing_columns = columns[2:] if lulc_column else columns[1:]
    
    print(f"üåä C·ªôt nh√£n l≈© (gi·ªØ nguy√™n): {flood_column}")
    print(f"üè† C·ªôt LULC (gi·ªØ nguy√™n): {lulc_column}")
    print(f"üîß S·ªë c·ªôt c·∫ßn chu·∫©n h√≥a: {len(processing_columns)}")
    
    # T√≠nh th·ªëng k√™ to√†n c·ª•c tr∆∞·ªõc (c·∫ßn cho minmax)
    print("üìà T√≠nh th·ªëng k√™ to√†n c·ª•c...")
    global_stats = {}
    
    if normalization_method == 'minmax':
        chunk_reader = pd.read_csv(input_file, chunksize=chunk_size*2, low_memory=False, engine='c', buffer_lines=50000)
        
        for col in processing_columns:
            global_stats[col] = {'min': float('inf'), 'max': float('-inf')}
        
        for chunk in chunk_reader:
            for col in processing_columns:
                if col in chunk.columns and pd.api.types.is_numeric_dtype(chunk[col]):
                    col_min = chunk[col].min()
                    col_max = chunk[col].max()
                    if not pd.isna(col_min):
                        global_stats[col]['min'] = min(global_stats[col]['min'], col_min)
                    if not pd.isna(col_max):
                        global_stats[col]['max'] = max(global_stats[col]['max'], col_max)
        
        print(f"‚úì ƒê√£ t√≠nh th·ªëng k√™ cho {len(global_stats)} c·ªôt")
    
    # X·ª≠ l√Ω file v·ªõi multiprocessing t·ªëi ∆∞u
    print("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω song song t·ªëi ∆∞u...")
    start_time = time.time()
    
    chunk_reader = pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, engine='c', buffer_lines=50000)
    total_processed = 0
    first_chunk = True
    
    # Batch processing l·ªõn ƒë·ªÉ t·ªëi ∆∞u RAM v√† CPU
    batch_size = n_workers * 3  # TƒÉng batch size
    chunk_batch = []
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        for i, chunk in enumerate(chunk_reader):
            # √Åp d·ª•ng th·ªëng k√™ to√†n c·ª•c cho minmax
            if normalization_method == 'minmax':
                for col in processing_columns:
                    if col in chunk.columns and col in global_stats:
                        min_val = global_stats[col]['min']
                        max_val = global_stats[col]['max']
                        if max_val != min_val and not (pd.isna(min_val) or pd.isna(max_val)):
                            chunk[col] = (chunk[col] - min_val) / (max_val - min_val)
                        else:
                            chunk[col] = 0
            
            # Chu·∫©n b·ªã args cho worker
            chunk_args = (chunk, processing_columns, flood_column, lulc_column, normalization_method)
            chunk_batch.append(chunk_args)
            
            # X·ª≠ l√Ω batch khi ƒë·∫ßy
            if len(chunk_batch) >= batch_size:
                futures = [executor.submit(process_chunk_normalization, args) for args in chunk_batch]
                
                # Thu th·∫≠p k·∫øt qu·∫£ v√† ghi file theo batch
                batch_results = []
                for future in as_completed(futures):
                    processed_chunk = future.result()
                    batch_results.append(processed_chunk)
                
                # Ghi t·∫•t c·∫£ k·∫øt qu·∫£ c·ªßa batch c√πng l√∫c ƒë·ªÉ t·ªëi ∆∞u I/O
                if batch_results:
                    combined_batch = pd.concat(batch_results, ignore_index=True)
                    
                    if first_chunk:
                        combined_batch.to_csv(output_file, index=False, mode='w')
                        first_chunk = False
                    else:
                        combined_batch.to_csv(output_file, index=False, mode='a', header=False)
                    
                    total_processed += len(combined_batch)
                
                # D·ªçn d·∫πp
                chunk_batch = []
                del batch_results
                import gc
                gc.collect()
                
                # Progress m·ªói 10 batch ƒë·ªÉ gi·∫£m overhead
                if i % (batch_size * 10) == 0:
                    elapsed = time.time() - start_time
                    speed = total_processed / elapsed if elapsed > 0 else 0
                    print(f"‚ö° {total_processed:,} d√≤ng | {speed:,.0f} d√≤ng/s")
        
        # X·ª≠ l√Ω batch cu·ªëi
        if chunk_batch:
            futures = [executor.submit(process_chunk_normalization, args) for args in chunk_batch]
            
            for future in as_completed(futures):
                processed_chunk = future.result()
                
                if first_chunk:
                    processed_chunk.to_csv(output_file, index=False, mode='w')
                    first_chunk = False
                else:
                    processed_chunk.to_csv(output_file, index=False, mode='a', header=False)
                
                total_processed += len(processed_chunk)
    
    elapsed_total = time.time() - start_time
    print(f"\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {total_processed:,} d√≤ng trong {elapsed_total:.1f} gi√¢y")
    print(f"‚ö° T·ªëc ƒë·ªô trung b√¨nh: {total_processed/elapsed_total:,.0f} d√≤ng/gi√¢y")
    print(f"üìÅ File k·∫øt qu·∫£: {output_file}")
    
    return True

def show_sample_output(output_file, num_rows=10):
    """
    Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u t·ª´ file k·∫øt qu·∫£
    """
    print(f"\nüìã === {num_rows} D√íNG ƒê·∫¶U C·ª¶A FILE {output_file} ===")
    
    try:
        # ƒê·ªçc file k·∫øt qu·∫£
        sample = pd.read_csv(output_file, nrows=num_rows)
        
        print(f"üìä K√≠ch th∆∞·ªõc file: {sample.shape[0]} d√≤ng (m·∫´u) √ó {sample.shape[1]} c·ªôt")
        print(f"üìã C√°c c·ªôt: {list(sample.columns)}")
        
        # Set display options for better formatting
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.float_format', '{:.6f}'.format)
        
        print(f"\nüìä D·ªÆ LI·ªÜU {num_rows} D√íNG ƒê·∫¶U:")
        print(sample.to_string(index=True))
        
        # Th·ªëng k√™ chu·∫©n h√≥a
        print(f"\nüìà TH·ªêNG K√ä SAU CHU·∫®N H√ìA:")
        numeric_cols = sample.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) > 0:
            print("üìä Kho·∫£ng gi√° tr·ªã c√°c c·ªôt s·ªë:")
            for col in numeric_cols:
                col_min = sample[col].min()
                col_max = sample[col].max()
                col_mean = sample[col].mean()
                
                # Ki·ªÉm tra xem c√≥ ƒë∆∞·ª£c chu·∫©n h√≥a ch∆∞a
                if col_min >= -0.01 and col_max <= 1.01:
                    status = "‚úÖ ƒê√£ chu·∫©n h√≥a"
                else:
                    status = "‚ö†Ô∏è Ch∆∞a chu·∫©n h√≥a"
                
                print(f"  {col}: [{col_min:.6f}, {col_max:.6f}] Mean={col_mean:.6f} {status}")
        
        # Ki·ªÉm tra missing values
        missing_info = sample.isnull().sum()
        missing_cols = missing_info[missing_info > 0]
        
        if len(missing_cols) > 0:
            print(f"\n‚ö†Ô∏è C·ªôt c√≤n missing values:")
            for col, count in missing_cols.items():
                pct = (count / len(sample)) * 100
                print(f"  {col}: {count} ({pct:.1f}%)")
        else:
            print(f"\n‚úÖ Kh√¥ng c√≤n missing values!")
        
        # Ki·ªÉm tra file size
        file_size = os.path.getsize(output_file) / (1024**3)  # GB
        print(f"\nüíæ K√≠ch th∆∞·ªõc file: {file_size:.2f} GB")
        
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")

# Ch∆∞∆°ng tr√¨nh ch√≠nh
if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n file CSV - H·ªó tr·ª£ c·∫£ file l·ªõn v√† nh·ªè
    file_path = r"C:\Users\Admin\Downloads\prj\Flood_point\merged_flood_point_merge_cleaned_balanced_reordered_nonlatlon.csv"
    
    # Ki·ªÉm tra k√≠ch th∆∞·ªõc file
    import os
    file_size_gb = os.path.getsize(file_path) / (1024**3) if os.path.exists(file_path) else 0
    
    print(f"üìÅ File: {os.path.basename(file_path)}")
    print(f"üíæ K√≠ch th∆∞·ªõc: {file_size_gb:.2f}GB")
    
    # Quy·∫øt ƒë·ªãnh ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ª±a tr√™n k√≠ch th∆∞·ªõc file
    if file_size_gb > 5:  # File l·ªõn h∆°n 5GB - d√πng multiprocessing
        print(f"\nüöÄ File l·ªõn ({file_size_gb:.1f}GB) - S·ª≠ d·ª•ng multiprocessing")
        
        # T·∫°o t√™n file output
        base_name = os.path.splitext(file_path)[0]
        output_file = f"{base_name}_normalized_multiprocessing.csv"
        
        # C·∫•u h√¨nh t·ªëi ∆∞u cho m√°y m·∫°nh
        n_workers = 20  # 20/24 cores
        chunk_size = 1000000  # 1M d√≤ng cho t·ªëi ∆∞u t·ªëc ƒë·ªô
        normalization_method = 'minmax'  # Ho·∫∑c 'standard'
        
        print(f"‚öôÔ∏è C·∫•u h√¨nh: {n_workers} workers, chunk {chunk_size:,} d√≤ng")
        
        # X·ª≠ l√Ω v·ªõi multiprocessing
        success = normalize_large_csv_parallel(
            input_file=file_path,
            output_file=output_file, 
            chunk_size=chunk_size,
            n_workers=n_workers,
            normalization_method=normalization_method
        )
        
        if success:
            print(f"\n‚úÖ MULTIPROCESSING HO√ÄN TH√ÄNH!")
            
            # Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£
            show_sample_output(output_file, num_rows=10)
            
            print(f"\nüéØ File k·∫øt qu·∫£: {output_file}")
            print("üëã Ch∆∞∆°ng tr√¨nh ho√†n th√†nh!")
        else:
            print("‚ùå C√≥ l·ªói trong qu√° tr√¨nh multiprocessing!")
            
    else:  # File nh·ªè - d√πng ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng
        print(f"\nüîß File nh·ªè ({file_size_gb:.1f}GB) - S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng")
        
        # Kh·ªüi t·∫°o normalizer
        normalizer = CSVDataNormalizer(file_path, chunk_size=1000000, n_workers=20)
        
        # T·∫£i d·ªØ li·ªáu
        if normalizer.load_data():
            print("\n" + "="*80)
            print("üéõÔ∏è T·ª∞ ƒê·ªòNG X·ª¨ L√ù T·∫§T C·∫¢ C√ÅC C·ªòT")
            print("=" * 80)
            
            # T·ª± ƒë·ªông x·ª≠ l√Ω t·∫•t c·∫£ c√°c c·ªôt v·ªõi ph∆∞∆°ng ph√°p minmax
            success = normalizer.process_all_columns(normalization_method='minmax')
            
            if success:
                # T·ª± ƒë·ªông l∆∞u k·∫øt qu·∫£
                output_path = normalizer.save_data()
                if output_path:
                    print(f"\n‚úÖ HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
                    
                    # Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu c·ªßa file k·∫øt qu·∫£
                    show_sample_output(output_path, num_rows=10)
                    
                    print(f"\nüéØ File ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}")
                    print("üëã Ch∆∞∆°ng tr√¨nh ho√†n th√†nh!")
                else:
                    print("‚ùå C√≥ l·ªói khi l∆∞u file!")
            else:
                print("‚ùå C√≥ l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω!")
        else:
            print("‚ùå Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu!")
    
    print("\nüëã Ch∆∞∆°ng tr√¨nh k·∫øt th√∫c!")